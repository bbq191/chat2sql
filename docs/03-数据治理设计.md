# é˜¶æ®µ3ï¼šæ•°æ®æ²»ç†è¯¦ç»†è®¾è®¡

## ğŸ“‹ é˜¶æ®µæ¦‚è¿°

### ç›®æ ‡ä¸ä»·å€¼
åœ¨ç”¨æˆ·ä½“ç³»åŸºç¡€ä¸Šï¼Œæ„å»ºå®Œæ•´çš„æ•°æ®æ²»ç†å¹³å°ï¼Œå®ç°å¤šæ•°æ®æºç®¡ç†ã€å…ƒæ•°æ®æ²»ç†ã€æ•°æ®è¡€ç¼˜åˆ†æã€æŸ¥è¯¢ç›‘æ§ç­‰ä¼ä¸šçº§æ•°æ®ç®¡ç†åŠŸèƒ½ã€‚

**æ ¸å¿ƒä»·å€¼ä¸»å¼ **ï¼š
- ç»Ÿä¸€ç®¡ç†å¤šä¸ªæ•°æ®æºï¼Œæ”¯æŒå¼‚æ„æ•°æ®åº“
- è‡ªåŠ¨å‘ç°å’Œç»´æŠ¤æ•°æ®åº“å…ƒæ•°æ®ä¿¡æ¯
- ä¸ºAIæä¾›ä¸°å¯Œå‡†ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå‡SQLç”Ÿæˆè´¨é‡
- å®Œæ•´çš„æ•°æ®è®¿é—®æƒé™æ§åˆ¶å’Œå®¡è®¡è¿½è¸ª
- æ™ºèƒ½åŒ–çš„æŸ¥è¯¢ä¼˜åŒ–å’Œæ€§èƒ½ç›‘æ§
- ä¼ä¸šçº§æ•°æ®æ²»ç†èƒ½åŠ›ï¼Œç¡®ä¿æ•°æ®åˆè§„

### æˆåŠŸæŒ‡æ ‡
- **æ•°æ®æºæ¥å…¥å®Œæ•´æ€§**ï¼šæ”¯æŒ90%+ä¸»æµæ•°æ®åº“ç±»å‹
- **å…ƒæ•°æ®åŒæ­¥å‡†ç¡®æ€§**ï¼šå…ƒæ•°æ®åŒæ­¥å‡†ç¡®ç‡ > 99%
- **SQLç”Ÿæˆè´¨é‡æå‡**ï¼šå‡†ç¡®ç‡ä»85%æå‡åˆ°95%+
- **æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–**ï¼šå¹³å‡å“åº”æ—¶é—´é™ä½50%+
- **æ•°æ®å®‰å…¨åˆè§„**ï¼š100%æ•°æ®è®¿é—®å¯è¿½æº¯å®¡è®¡

## ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡

### æ•°æ®æ²»ç†æ¶æ„å›¾

```mermaid
graph TB
    A[ç”¨æˆ·ç•Œé¢] --> B[æ•°æ®æºç®¡ç†æœåŠ¡]
    A --> C[å…ƒæ•°æ®ç®¡ç†æœåŠ¡]
    A --> D[æŸ¥è¯¢æ‰§è¡Œå¼•æ“]
    A --> E[æ•°æ®è¡€ç¼˜æœåŠ¡]
    A --> F[æƒé™æ§åˆ¶æœåŠ¡]
    
    B --> G[è¿æ¥æ± ç®¡ç†å™¨]
    C --> H[å…ƒæ•°æ®å­˜å‚¨]
    D --> I[æŸ¥è¯¢ä¼˜åŒ–å™¨]
    E --> J[è¡€ç¼˜åˆ†æå¼•æ“]
    F --> K[æƒé™ç­–ç•¥å¼•æ“]
    
    G --> L[(MySQL)]
    G --> M[(PostgreSQL)]
    G --> N[(Oracle)]
    G --> O[(SQL Server)]
    G --> P[(ClickHouse)]
    G --> Q[(MongoDB)]
    
    subgraph "å…ƒæ•°æ®åŒæ­¥æµç¨‹"
        R[å®šæ—¶ä»»åŠ¡] --> S[Schemaæ‰«æ]
        S --> T[å·®å¼‚æ£€æµ‹]
        T --> U[å…ƒæ•°æ®æ›´æ–°]
        U --> V[AIä¸Šä¸‹æ–‡æ›´æ–°]
    end
    
    subgraph "æŸ¥è¯¢æ‰§è¡Œæµç¨‹"
        W[SQLè§£æ] --> X[æƒé™æ£€æŸ¥]
        X --> Y[æŸ¥è¯¢ä¼˜åŒ–]
        Y --> Z[æ‰§è¡Œç›‘æ§]
        Z --> AA[ç»“æœç¼“å­˜]
    end
```

### æ ¸å¿ƒæ¨¡å—è®¾è®¡

#### 1. æ•°æ®æºç®¡ç†æ¨¡å—
- **å¤šæ•°æ®åº“é€‚é…å™¨**ï¼šç»Ÿä¸€çš„æ•°æ®åº“è®¿é—®æ¥å£
- **è¿æ¥æ± ç®¡ç†**ï¼šæ™ºèƒ½è¿æ¥æ± ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡
- **å¥åº·æ£€æŸ¥**ï¼šå®æ—¶ç›‘æ§æ•°æ®æºçŠ¶æ€
- **é…ç½®ç®¡ç†**ï¼šå®‰å…¨çš„æ•°æ®æºé…ç½®å­˜å‚¨

#### 2. å…ƒæ•°æ®ç®¡ç†æ¨¡å—
- **Schemaå‘ç°**ï¼šè‡ªåŠ¨æ‰«ææ•°æ®åº“ç»“æ„
- **å…ƒæ•°æ®åŒæ­¥**ï¼šå¢é‡åŒæ­¥å…ƒæ•°æ®å˜æ›´
- **æ•°æ®å­—å…¸**ï¼šç»´æŠ¤ä¸šåŠ¡å­—æ®µå«ä¹‰
- **ç‰ˆæœ¬ç®¡ç†**ï¼šè·Ÿè¸ªSchemaå˜æ›´å†å²

#### 3. æŸ¥è¯¢æ‰§è¡Œå¼•æ“
- **SQLè§£æå™¨**ï¼šè§£æå’ŒéªŒè¯SQLè¯­å¥
- **æŸ¥è¯¢ä¼˜åŒ–å™¨**ï¼šåŸºäºç»Ÿè®¡ä¿¡æ¯ä¼˜åŒ–æŸ¥è¯¢
- **æ‰§è¡Œç›‘æ§**ï¼šå®æ—¶ç›‘æ§æŸ¥è¯¢æ‰§è¡ŒçŠ¶æ€
- **ç»“æœç¼“å­˜**ï¼šæ™ºèƒ½ç¼“å­˜æŸ¥è¯¢ç»“æœ

#### 4. æ•°æ®è¡€ç¼˜æœåŠ¡
- **è¡€ç¼˜åˆ†æ**ï¼šåˆ†æè¡¨é—´å…³ç³»å’Œæ•°æ®æµ
- **å½±å“åˆ†æ**ï¼šè¯„ä¼°Schemaå˜æ›´å½±å“
- **ä¾èµ–è¿½è¸ª**ï¼šè·Ÿè¸ªæŸ¥è¯¢å’Œè¡¨çš„ä¾èµ–å…³ç³»

## ğŸ“Š æ•°æ®åº“è¯¦ç»†è®¾è®¡

### å…ƒæ•°æ®ç®¡ç†æ ¸å¿ƒè¡¨ç»“æ„

```sql
-- 1. æ•°æ®æºå®šä¹‰è¡¨
CREATE TABLE data_sources (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    name VARCHAR(100) NOT NULL UNIQUE,
    display_name VARCHAR(200) NOT NULL,
    description TEXT,
    db_type VARCHAR(50) NOT NULL, -- mysql, postgresql, oracle, sqlserver, clickhouse, mongodb
    host VARCHAR(255) NOT NULL,
    port INTEGER NOT NULL,
    database_name VARCHAR(100) NOT NULL,
    username VARCHAR(100),
    password_encrypted TEXT, -- åŠ å¯†å­˜å‚¨å¯†ç 
    connection_params JSON, -- é¢å¤–è¿æ¥å‚æ•°
    max_connections INTEGER DEFAULT 10,
    connection_timeout INTEGER DEFAULT 30, -- ç§’
    query_timeout INTEGER DEFAULT 300, -- ç§’
    ssl_enabled BOOLEAN DEFAULT FALSE,
    ssl_config JSON,
    status TINYINT DEFAULT 1 COMMENT '1:active, 0:inactive, -1:error',
    last_health_check DATETIME,
    health_check_error TEXT,
    created_by TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (created_by) REFERENCES users(id)
);

-- 2. æ•°æ®åº“Schemaä¿¡æ¯è¡¨
CREATE TABLE database_schemas (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    data_source_id TEXT NOT NULL,
    schema_name VARCHAR(100) NOT NULL,
    schema_type VARCHAR(50) DEFAULT 'database', -- database, schema, namespace
    description TEXT,
    table_count INTEGER DEFAULT 0,
    total_size_bytes BIGINT DEFAULT 0,
    last_synced_at DATETIME,
    sync_status VARCHAR(20) DEFAULT 'pending', -- pending, syncing, completed, failed
    sync_error TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (data_source_id) REFERENCES data_sources(id) ON DELETE CASCADE,
    UNIQUE(data_source_id, schema_name)
);

-- 3. æ•°æ®è¡¨å…ƒä¿¡æ¯è¡¨
CREATE TABLE table_metadata (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    schema_id TEXT NOT NULL,
    table_name VARCHAR(200) NOT NULL,
    table_type VARCHAR(50) DEFAULT 'table', -- table, view, materialized_view
    display_name VARCHAR(300),
    business_description TEXT, -- ä¸šåŠ¡å«ä¹‰æè¿°
    technical_description TEXT, -- æŠ€æœ¯æè¿°
    owner VARCHAR(100),
    tags JSON, -- æ ‡ç­¾ï¼š['pii', 'sensitive', 'dimension', 'fact']
    row_count BIGINT DEFAULT 0,
    size_bytes BIGINT DEFAULT 0,
    partitioned BOOLEAN DEFAULT FALSE,
    partition_info JSON,
    created_at_db DATETIME, -- åœ¨æ•°æ®åº“ä¸­çš„åˆ›å»ºæ—¶é—´
    updated_at_db DATETIME, -- åœ¨æ•°æ®åº“ä¸­çš„æ›´æ–°æ—¶é—´
    last_analyzed DATETIME, -- æœ€ååˆ†ææ—¶é—´
    data_freshness_hours INTEGER, -- æ•°æ®æ–°é²œåº¦ï¼ˆå°æ—¶ï¼‰
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (schema_id) REFERENCES database_schemas(id) ON DELETE CASCADE,
    UNIQUE(schema_id, table_name)
);

-- 4. å­—æ®µå…ƒä¿¡æ¯è¡¨
CREATE TABLE column_metadata (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    table_id TEXT NOT NULL,
    column_name VARCHAR(200) NOT NULL,
    ordinal_position INTEGER NOT NULL,
    data_type VARCHAR(100) NOT NULL,
    max_length INTEGER,
    numeric_precision INTEGER,
    numeric_scale INTEGER,
    is_nullable BOOLEAN DEFAULT TRUE,
    column_default TEXT,
    is_primary_key BOOLEAN DEFAULT FALSE,
    is_foreign_key BOOLEAN DEFAULT FALSE,
    foreign_key_table TEXT,
    foreign_key_column TEXT,
    is_unique BOOLEAN DEFAULT FALSE,
    is_indexed BOOLEAN DEFAULT FALSE,
    display_name VARCHAR(300),
    business_description TEXT, -- ä¸šåŠ¡å«ä¹‰
    data_classification VARCHAR(50), -- public, internal, confidential, restricted
    data_category VARCHAR(50), -- identifier, attribute, metric, dimension
    sample_values JSON, -- ç¤ºä¾‹å€¼
    value_range JSON, -- å€¼åŸŸèŒƒå›´
    enum_values JSON, -- æšä¸¾å€¼
    regex_pattern TEXT, -- æ•°æ®æ ¼å¼æ­£åˆ™
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (table_id) REFERENCES table_metadata(id) ON DELETE CASCADE,
    UNIQUE(table_id, column_name)
);

-- 5. è¡¨å…³ç³»å…ƒä¿¡æ¯è¡¨
CREATE TABLE table_relationships (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    source_table_id TEXT NOT NULL,
    target_table_id TEXT NOT NULL,
    relationship_type VARCHAR(50) NOT NULL, -- foreign_key, reference, derived, aggregation
    source_columns JSON NOT NULL, -- ["column1", "column2"]
    target_columns JSON NOT NULL, -- ["ref_column1", "ref_column2"]
    relationship_strength DECIMAL(3,2) DEFAULT 1.00, -- å…³ç³»å¼ºåº¦ 0.0-1.0
    constraint_name VARCHAR(200),
    is_enforced BOOLEAN DEFAULT TRUE,
    description TEXT,
    discovered_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    confidence_score DECIMAL(3,2) DEFAULT 1.00, -- ç½®ä¿¡åº¦
    FOREIGN KEY (source_table_id) REFERENCES table_metadata(id) ON DELETE CASCADE,
    FOREIGN KEY (target_table_id) REFERENCES table_metadata(id) ON DELETE CASCADE,
    UNIQUE(source_table_id, target_table_id, relationship_type, constraint_name)
);

-- 6. æŸ¥è¯¢æ‰§è¡Œå†å²è¡¨
CREATE TABLE query_execution_history (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    user_id TEXT NOT NULL,
    session_id TEXT,
    data_source_id TEXT NOT NULL,
    sql_query TEXT NOT NULL,
    sql_hash VARCHAR(64) NOT NULL, -- SQLçš„å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡ç»Ÿè®¡
    query_type VARCHAR(20) DEFAULT 'SELECT', -- SELECT, INSERT, UPDATE, DELETE
    execution_status VARCHAR(20) DEFAULT 'pending', -- pending, running, completed, failed, cancelled
    start_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    end_time DATETIME,
    execution_time_ms INTEGER,
    rows_affected INTEGER,
    rows_returned INTEGER,
    bytes_scanned BIGINT,
    error_message TEXT,
    query_plan JSON, -- æ‰§è¡Œè®¡åˆ’
    performance_stats JSON, -- æ€§èƒ½ç»Ÿè®¡
    client_ip VARCHAR(45),
    user_agent TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (session_id) REFERENCES chat_sessions(id),
    FOREIGN KEY (data_source_id) REFERENCES data_sources(id)
);

-- 7. æ•°æ®è¡€ç¼˜å…³ç³»è¡¨
CREATE TABLE data_lineage (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    source_table_id TEXT NOT NULL,
    target_table_id TEXT NOT NULL,
    source_columns JSON, -- æºå­—æ®µåˆ—è¡¨
    target_columns JSON, -- ç›®æ ‡å­—æ®µåˆ—è¡¨
    transformation_type VARCHAR(50), -- direct_copy, aggregation, join, calculation, filter
    transformation_logic TEXT, -- è½¬æ¢é€»è¾‘æè¿°
    transformation_sql TEXT, -- è½¬æ¢SQL
    lineage_type VARCHAR(50) DEFAULT 'table', -- table, column, query
    confidence_score DECIMAL(3,2) DEFAULT 0.80,
    discovered_by VARCHAR(50) DEFAULT 'system', -- system, manual, analysis
    impact_level VARCHAR(20) DEFAULT 'medium', -- low, medium, high, critical
    last_verified DATETIME DEFAULT CURRENT_TIMESTAMP,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (source_table_id) REFERENCES table_metadata(id) ON DELETE CASCADE,
    FOREIGN KEY (target_table_id) REFERENCES table_metadata(id) ON DELETE CASCADE
);

-- 8. æ•°æ®è´¨é‡è§„åˆ™è¡¨
CREATE TABLE data_quality_rules (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    table_id TEXT NOT NULL,
    column_id TEXT,
    rule_name VARCHAR(100) NOT NULL,
    rule_type VARCHAR(50) NOT NULL, -- not_null, unique, range, format, custom
    rule_config JSON NOT NULL, -- è§„åˆ™é…ç½®
    severity VARCHAR(20) DEFAULT 'warning', -- info, warning, error, critical
    is_active BOOLEAN DEFAULT TRUE,
    created_by TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (table_id) REFERENCES table_metadata(id) ON DELETE CASCADE,
    FOREIGN KEY (column_id) REFERENCES column_metadata(id) ON DELETE CASCADE,
    FOREIGN KEY (created_by) REFERENCES users(id)
);

-- 9. æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœè¡¨
CREATE TABLE data_quality_results (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    rule_id TEXT NOT NULL,
    check_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(20) NOT NULL, -- passed, failed, error
    passed_count BIGINT DEFAULT 0,
    failed_count BIGINT DEFAULT 0,
    error_details TEXT,
    sample_failures JSON, -- å¤±è´¥æ ·æœ¬æ•°æ®
    score DECIMAL(5,2), -- è´¨é‡å¾—åˆ† 0-100
    trend VARCHAR(20), -- improving, stable, declining
    FOREIGN KEY (rule_id) REFERENCES data_quality_rules(id) ON DELETE CASCADE
);

-- 10. æ•°æ®è®¿é—®æƒé™è¡¨
CREATE TABLE data_access_permissions (
    id TEXT PRIMARY KEY DEFAULT (lower(hex(randomblob(16)))),
    user_id TEXT,
    role_id TEXT,
    data_source_id TEXT,
    schema_id TEXT,
    table_id TEXT,
    permission_type VARCHAR(50) NOT NULL, -- read, write, execute, admin
    access_level VARCHAR(50) DEFAULT 'full', -- full, filtered, masked, denied
    filter_condition TEXT, -- è¡Œçº§æƒé™è¿‡æ»¤æ¡ä»¶
    mask_columns JSON, -- éœ€è¦è„±æ•çš„å­—æ®µ
    granted_by TEXT NOT NULL,
    granted_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    expires_at DATETIME,
    is_active BOOLEAN DEFAULT TRUE,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
    FOREIGN KEY (role_id) REFERENCES roles(id) ON DELETE CASCADE,
    FOREIGN KEY (data_source_id) REFERENCES data_sources(id) ON DELETE CASCADE,
    FOREIGN KEY (schema_id) REFERENCES database_schemas(id) ON DELETE CASCADE,
    FOREIGN KEY (table_id) REFERENCES table_metadata(id) ON DELETE CASCADE,
    FOREIGN KEY (granted_by) REFERENCES users(id),
    CHECK ((user_id IS NOT NULL) OR (role_id IS NOT NULL))
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_data_sources_status ON data_sources(status);
CREATE INDEX idx_database_schemas_data_source_id ON database_schemas(data_source_id);
CREATE INDEX idx_table_metadata_schema_id ON table_metadata(schema_id);
CREATE INDEX idx_table_metadata_table_name ON table_metadata(table_name);
CREATE INDEX idx_column_metadata_table_id ON column_metadata(table_id);
CREATE INDEX idx_column_metadata_column_name ON column_metadata(column_name);
CREATE INDEX idx_table_relationships_source_table_id ON table_relationships(source_table_id);
CREATE INDEX idx_table_relationships_target_table_id ON table_relationships(target_table_id);
CREATE INDEX idx_query_execution_history_user_id ON query_execution_history(user_id);
CREATE INDEX idx_query_execution_history_data_source_id ON query_execution_history(data_source_id);
CREATE INDEX idx_query_execution_history_sql_hash ON query_execution_history(sql_hash);
CREATE INDEX idx_query_execution_history_start_time ON query_execution_history(start_time);
CREATE INDEX idx_data_lineage_source_table_id ON data_lineage(source_table_id);
CREATE INDEX idx_data_lineage_target_table_id ON data_lineage(target_table_id);
CREATE INDEX idx_data_quality_rules_table_id ON data_quality_rules(table_id);
CREATE INDEX idx_data_quality_results_rule_id ON data_quality_results(rule_id);
CREATE INDEX idx_data_quality_results_check_time ON data_quality_results(check_time);
CREATE INDEX idx_data_access_permissions_user_id ON data_access_permissions(user_id);
CREATE INDEX idx_data_access_permissions_data_source_id ON data_access_permissions(data_source_id);
```

### åˆå§‹åŒ–æ•°æ®

```sql
-- æ’å…¥é»˜è®¤æ•°æ®æºç±»å‹é…ç½®
INSERT INTO system_config (key, value, description) VALUES
('supported_db_types', '["mysql", "postgresql", "oracle", "sqlserver", "clickhouse", "mongodb", "sqlite", "hive", "spark", "redshift"]', 'æ”¯æŒçš„æ•°æ®åº“ç±»å‹'),
('default_connection_timeout', '30', 'é»˜è®¤è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰'),
('default_query_timeout', '300', 'é»˜è®¤æŸ¥è¯¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰'),
('metadata_sync_interval', '3600', 'å…ƒæ•°æ®åŒæ­¥é—´éš”ï¼ˆç§’ï¼‰'),
('max_query_result_rows', '10000', 'æŸ¥è¯¢ç»“æœæœ€å¤§è¡Œæ•°'),
('enable_query_cache', 'true', 'æ˜¯å¦å¯ç”¨æŸ¥è¯¢ç¼“å­˜'),
('cache_ttl_seconds', '86400', 'ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰');

-- æ’å…¥æ•°æ®åˆ†ç±»æšä¸¾
INSERT INTO system_config (key, value, description) VALUES
('data_classifications', '["public", "internal", "confidential", "restricted"]', 'æ•°æ®åˆ†ç±»ç­‰çº§'),
('data_categories', '["identifier", "attribute", "metric", "dimension", "measure"]', 'æ•°æ®ç±»åˆ«'),
('table_tags', '["pii", "sensitive", "dimension", "fact", "staging", "mart", "raw"]', 'è¡¨æ ‡ç­¾'),
('relationship_types', '["foreign_key", "reference", "derived", "aggregation", "lookup"]', 'è¡¨å…³ç³»ç±»å‹');
```

## ğŸ”§ è¯¦ç»†æŠ€æœ¯å®ç°

### 1. æ•°æ®æºç®¡ç†æœåŠ¡

```go
// internal/service/datasource_service.go
package service

import (
    "context"
    "crypto/aes"
    "crypto/cipher"
    "crypto/rand"
    "database/sql"
    "encoding/base64"
    "fmt"
    "io"
    "sync"
    "time"
)

type DataSourceService struct {
    repo           repository.DataSourceRepository
    connectionPool map[string]*ConnectionPool
    poolMutex      sync.RWMutex
    encryptionKey  []byte
}

type ConnectionPool struct {
    DataSource *model.DataSource
    Connections chan *sql.DB
    Active     map[*sql.DB]bool
    Mutex      sync.Mutex
    LastUsed   time.Time
    HealthStatus string
}

type DataSourceRequest struct {
    Name           string            `json:"name" validate:"required"`
    DisplayName    string            `json:"display_name" validate:"required"`
    Description    string            `json:"description"`
    DBType         string            `json:"db_type" validate:"required,oneof=mysql postgresql oracle sqlserver clickhouse mongodb"`
    Host           string            `json:"host" validate:"required"`
    Port           int               `json:"port" validate:"required,min=1,max=65535"`
    DatabaseName   string            `json:"database_name" validate:"required"`
    Username       string            `json:"username" validate:"required"`
    Password       string            `json:"password" validate:"required"`
    ConnectionParams map[string]interface{} `json:"connection_params"`
    MaxConnections int               `json:"max_connections" validate:"min=1,max=100"`
    SSLEnabled     bool              `json:"ssl_enabled"`
    SSLConfig      map[string]interface{} `json:"ssl_config"`
}

func NewDataSourceService(repo repository.DataSourceRepository, encryptionKey string) *DataSourceService {
    return &DataSourceService{
        repo:           repo,
        connectionPool: make(map[string]*ConnectionPool),
        encryptionKey:  []byte(encryptionKey),
    }
}

// CreateDataSource åˆ›å»ºæ•°æ®æº
func (ds *DataSourceService) CreateDataSource(ctx context.Context, req *DataSourceRequest, createdBy string) (*model.DataSource, error) {
    // 1. éªŒè¯è¿æ¥
    if err := ds.testConnection(ctx, req); err != nil {
        return nil, fmt.Errorf("connection test failed: %w", err)
    }
    
    // 2. åŠ å¯†å¯†ç 
    encryptedPassword, err := ds.encryptPassword(req.Password)
    if err != nil {
        return nil, fmt.Errorf("encrypt password: %w", err)
    }
    
    // 3. åˆ›å»ºæ•°æ®æºå¯¹è±¡
    dataSource := &model.DataSource{
        Name:             req.Name,
        DisplayName:      req.DisplayName,
        Description:      req.Description,
        DBType:           req.DBType,
        Host:             req.Host,
        Port:             req.Port,
        DatabaseName:     req.DatabaseName,
        Username:         req.Username,
        PasswordEncrypted: encryptedPassword,
        ConnectionParams: req.ConnectionParams,
        MaxConnections:   req.MaxConnections,
        SSLEnabled:       req.SSLEnabled,
        SSLConfig:        req.SSLConfig,
        Status:           model.DataSourceStatusActive,
        CreatedBy:        createdBy,
        CreatedAt:        time.Now(),
        UpdatedAt:        time.Now(),
    }
    
    // 4. ä¿å­˜åˆ°æ•°æ®åº“
    if err := ds.repo.Create(ctx, dataSource); err != nil {
        return nil, fmt.Errorf("create data source: %w", err)
    }
    
    // 5. åˆå§‹åŒ–è¿æ¥æ± 
    if err := ds.initializeConnectionPool(dataSource); err != nil {
        return nil, fmt.Errorf("initialize connection pool: %w", err)
    }
    
    // 6. è§¦å‘å…ƒæ•°æ®åŒæ­¥
    go ds.triggerMetadataSync(dataSource.ID)
    
    return dataSource, nil
}

// GetConnection è·å–æ•°æ®åº“è¿æ¥
func (ds *DataSourceService) GetConnection(ctx context.Context, dataSourceID string) (*sql.DB, error) {
    ds.poolMutex.RLock()
    pool, exists := ds.connectionPool[dataSourceID]
    ds.poolMutex.RUnlock()
    
    if !exists {
        // æ‡’åŠ è½½è¿æ¥æ± 
        dataSource, err := ds.repo.GetByID(ctx, dataSourceID)
        if err != nil {
            return nil, fmt.Errorf("get data source: %w", err)
        }
        
        if err := ds.initializeConnectionPool(dataSource); err != nil {
            return nil, fmt.Errorf("initialize connection pool: %w", err)
        }
        
        pool = ds.connectionPool[dataSourceID]
    }
    
    // ä»è¿æ¥æ± è·å–è¿æ¥
    select {
    case conn := <-pool.Connections:
        pool.Mutex.Lock()
        pool.Active[conn] = true
        pool.LastUsed = time.Now()
        pool.Mutex.Unlock()
        return conn, nil
    case <-time.After(30 * time.Second):
        return nil, fmt.Errorf("connection timeout")
    }
}

// ReleaseConnection é‡Šæ”¾æ•°æ®åº“è¿æ¥
func (ds *DataSourceService) ReleaseConnection(dataSourceID string, conn *sql.DB) {
    ds.poolMutex.RLock()
    pool, exists := ds.connectionPool[dataSourceID]
    ds.poolMutex.RUnlock()
    
    if !exists {
        conn.Close()
        return
    }
    
    pool.Mutex.Lock()
    delete(pool.Active, conn)
    pool.Mutex.Unlock()
    
    // æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
    if err := conn.Ping(); err != nil {
        conn.Close()
        return
    }
    
    // å½’è¿˜åˆ°è¿æ¥æ± 
    select {
    case pool.Connections <- conn:
    default:
        // è¿æ¥æ± å·²æ»¡ï¼Œç›´æ¥å…³é—­
        conn.Close()
    }
}

// TestConnection æµ‹è¯•æ•°æ®æºè¿æ¥
func (ds *DataSourceService) TestConnection(ctx context.Context, dataSourceID string) error {
    dataSource, err := ds.repo.GetByID(ctx, dataSourceID)
    if err != nil {
        return fmt.Errorf("get data source: %w", err)
    }
    
    // è§£å¯†å¯†ç 
    password, err := ds.decryptPassword(dataSource.PasswordEncrypted)
    if err != nil {
        return fmt.Errorf("decrypt password: %w", err)
    }
    
    // æ„å»ºè¿æ¥å­—ç¬¦ä¸²
    dsn := ds.buildConnectionString(dataSource, password)
    
    // æµ‹è¯•è¿æ¥
    db, err := sql.Open(dataSource.DBType, dsn)
    if err != nil {
        return fmt.Errorf("open connection: %w", err)
    }
    defer db.Close()
    
    ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
    defer cancel()
    
    if err := db.PingContext(ctx); err != nil {
        return fmt.Errorf("ping database: %w", err)
    }
    
    return nil
}

// ListDataSources åˆ—å‡ºæ•°æ®æº
func (ds *DataSourceService) ListDataSources(ctx context.Context, userID string) ([]*model.DataSource, error) {
    // æ ¹æ®ç”¨æˆ·æƒé™è¿‡æ»¤æ•°æ®æº
    dataSources, err := ds.repo.ListByUser(ctx, userID)
    if err != nil {
        return nil, fmt.Errorf("list data sources: %w", err)
    }
    
    // ç§»é™¤æ•æ„Ÿä¿¡æ¯
    for _, dataSource := range dataSources {
        dataSource.PasswordEncrypted = ""
    }
    
    return dataSources, nil
}

// UpdateDataSource æ›´æ–°æ•°æ®æº
func (ds *DataSourceService) UpdateDataSource(ctx context.Context, id string, req *DataSourceRequest) error {
    // 1. è·å–ç°æœ‰æ•°æ®æº
    existing, err := ds.repo.GetByID(ctx, id)
    if err != nil {
        return fmt.Errorf("get existing data source: %w", err)
    }
    
    // 2. å¦‚æœå¯†ç æˆ–è¿æ¥ä¿¡æ¯å˜æ›´ï¼Œæµ‹è¯•æ–°è¿æ¥
    if req.Password != "" || req.Host != existing.Host || req.Port != existing.Port {
        if err := ds.testConnection(ctx, req); err != nil {
            return fmt.Errorf("connection test failed: %w", err)
        }
    }
    
    // 3. æ›´æ–°æ•°æ®æºä¿¡æ¯
    if req.Password != "" {
        encryptedPassword, err := ds.encryptPassword(req.Password)
        if err != nil {
            return fmt.Errorf("encrypt password: %w", err)
        }
        existing.PasswordEncrypted = encryptedPassword
    }
    
    existing.DisplayName = req.DisplayName
    existing.Description = req.Description
    existing.Host = req.Host
    existing.Port = req.Port
    existing.DatabaseName = req.DatabaseName
    existing.Username = req.Username
    existing.ConnectionParams = req.ConnectionParams
    existing.MaxConnections = req.MaxConnections
    existing.SSLEnabled = req.SSLEnabled
    existing.SSLConfig = req.SSLConfig
    existing.UpdatedAt = time.Now()
    
    // 4. ä¿å­˜æ›´æ–°
    if err := ds.repo.Update(ctx, existing); err != nil {
        return fmt.Errorf("update data source: %w", err)
    }
    
    // 5. é‡æ–°åˆå§‹åŒ–è¿æ¥æ± 
    ds.closeConnectionPool(id)
    if err := ds.initializeConnectionPool(existing); err != nil {
        return fmt.Errorf("reinitialize connection pool: %w", err)
    }
    
    return nil
}

// DeleteDataSource åˆ é™¤æ•°æ®æº
func (ds *DataSourceService) DeleteDataSource(ctx context.Context, id string) error {
    // 1. å…³é—­è¿æ¥æ± 
    ds.closeConnectionPool(id)
    
    // 2. åˆ é™¤æ•°æ®æºï¼ˆçº§è”åˆ é™¤ç›¸å…³å…ƒæ•°æ®ï¼‰
    if err := ds.repo.Delete(ctx, id); err != nil {
        return fmt.Errorf("delete data source: %w", err)
    }
    
    return nil
}

// ç§æœ‰æ–¹æ³•

func (ds *DataSourceService) testConnection(ctx context.Context, req *DataSourceRequest) error {
    dsn := ds.buildConnectionStringFromRequest(req)
    
    db, err := sql.Open(req.DBType, dsn)
    if err != nil {
        return err
    }
    defer db.Close()
    
    ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
    defer cancel()
    
    return db.PingContext(ctx)
}

func (ds *DataSourceService) buildConnectionString(dataSource *model.DataSource, password string) string {
    switch dataSource.DBType {
    case "mysql":
        return fmt.Sprintf("%s:%s@tcp(%s:%d)/%s?parseTime=true&charset=utf8mb4",
            dataSource.Username, password, dataSource.Host, dataSource.Port, dataSource.DatabaseName)
    case "postgresql":
        return fmt.Sprintf("host=%s port=%d user=%s password=%s dbname=%s sslmode=disable",
            dataSource.Host, dataSource.Port, dataSource.Username, password, dataSource.DatabaseName)
    case "sqlserver":
        return fmt.Sprintf("server=%s;port=%d;user id=%s;password=%s;database=%s",
            dataSource.Host, dataSource.Port, dataSource.Username, password, dataSource.DatabaseName)
    case "oracle":
        return fmt.Sprintf("%s/%s@%s:%d/%s",
            dataSource.Username, password, dataSource.Host, dataSource.Port, dataSource.DatabaseName)
    default:
        return ""
    }
}

func (ds *DataSourceService) buildConnectionStringFromRequest(req *DataSourceRequest) string {
    switch req.DBType {
    case "mysql":
        return fmt.Sprintf("%s:%s@tcp(%s:%d)/%s?parseTime=true&charset=utf8mb4",
            req.Username, req.Password, req.Host, req.Port, req.DatabaseName)
    case "postgresql":
        return fmt.Sprintf("host=%s port=%d user=%s password=%s dbname=%s sslmode=disable",
            req.Host, req.Port, req.Username, req.Password, req.DatabaseName)
    case "sqlserver":
        return fmt.Sprintf("server=%s;port=%d;user id=%s;password=%s;database=%s",
            req.Host, req.Port, req.Username, req.Password, req.DatabaseName)
    case "oracle":
        return fmt.Sprintf("%s/%s@%s:%d/%s",
            req.Username, req.Password, req.Host, req.Port, req.DatabaseName)
    default:
        return ""
    }
}

func (ds *DataSourceService) initializeConnectionPool(dataSource *model.DataSource) error {
    password, err := ds.decryptPassword(dataSource.PasswordEncrypted)
    if err != nil {
        return fmt.Errorf("decrypt password: %w", err)
    }
    
    dsn := ds.buildConnectionString(dataSource, password)
    
    pool := &ConnectionPool{
        DataSource:  dataSource,
        Connections: make(chan *sql.DB, dataSource.MaxConnections),
        Active:      make(map[*sql.DB]bool),
        LastUsed:    time.Now(),
        HealthStatus: "healthy",
    }
    
    // é¢„åˆ›å»ºè¿æ¥
    for i := 0; i < dataSource.MaxConnections/2; i++ {
        conn, err := sql.Open(dataSource.DBType, dsn)
        if err != nil {
            return fmt.Errorf("create connection: %w", err)
        }
        
        if err := conn.Ping(); err != nil {
            conn.Close()
            return fmt.Errorf("ping connection: %w", err)
        }
        
        pool.Connections <- conn
    }
    
    ds.poolMutex.Lock()
    ds.connectionPool[dataSource.ID] = pool
    ds.poolMutex.Unlock()
    
    return nil
}

func (ds *DataSourceService) closeConnectionPool(dataSourceID string) {
    ds.poolMutex.Lock()
    pool, exists := ds.connectionPool[dataSourceID]
    if exists {
        delete(ds.connectionPool, dataSourceID)
    }
    ds.poolMutex.Unlock()
    
    if !exists {
        return
    }
    
    // å…³é—­æ‰€æœ‰è¿æ¥
    close(pool.Connections)
    for conn := range pool.Connections {
        conn.Close()
    }
    
    pool.Mutex.Lock()
    for conn := range pool.Active {
        conn.Close()
    }
    pool.Mutex.Unlock()
}

func (ds *DataSourceService) encryptPassword(password string) (string, error) {
    block, err := aes.NewCipher(ds.encryptionKey)
    if err != nil {
        return "", err
    }
    
    gcm, err := cipher.NewGCM(block)
    if err != nil {
        return "", err
    }
    
    nonce := make([]byte, gcm.NonceSize())
    if _, err := io.ReadFull(rand.Reader, nonce); err != nil {
        return "", err
    }
    
    ciphertext := gcm.Seal(nonce, nonce, []byte(password), nil)
    return base64.StdEncoding.EncodeToString(ciphertext), nil
}

func (ds *DataSourceService) decryptPassword(encryptedPassword string) (string, error) {
    data, err := base64.StdEncoding.DecodeString(encryptedPassword)
    if err != nil {
        return "", err
    }
    
    block, err := aes.NewCipher(ds.encryptionKey)
    if err != nil {
        return "", err
    }
    
    gcm, err := cipher.NewGCM(block)
    if err != nil {
        return "", err
    }
    
    nonceSize := gcm.NonceSize()
    if len(data) < nonceSize {
        return "", fmt.Errorf("ciphertext too short")
    }
    
    nonce, ciphertext := data[:nonceSize], data[nonceSize:]
    plaintext, err := gcm.Open(nil, nonce, ciphertext, nil)
    if err != nil {
        return "", err
    }
    
    return string(plaintext), nil
}

func (ds *DataSourceService) triggerMetadataSync(dataSourceID string) {
    // è§¦å‘å…ƒæ•°æ®åŒæ­¥ä»»åŠ¡
    // è¿™é‡Œå¯ä»¥å‘é€åˆ°æ¶ˆæ¯é˜Ÿåˆ—æˆ–ç›´æ¥è°ƒç”¨å…ƒæ•°æ®æœåŠ¡
    log.Printf("Triggering metadata sync for data source: %s", dataSourceID)
}

// å®šæœŸå¥åº·æ£€æŸ¥
func (ds *DataSourceService) StartHealthCheck() {
    ticker := time.NewTicker(5 * time.Minute)
    go func() {
        for range ticker.C {
            ds.performHealthCheck()
        }
    }()
}

func (ds *DataSourceService) performHealthCheck() {
    ds.poolMutex.RLock()
    pools := make([]*ConnectionPool, 0, len(ds.connectionPool))
    for _, pool := range ds.connectionPool {
        pools = append(pools, pool)
    }
    ds.poolMutex.RUnlock()
    
    for _, pool := range pools {
        go ds.checkPoolHealth(pool)
    }
}

func (ds *DataSourceService) checkPoolHealth(pool *ConnectionPool) {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    // ä»æ± ä¸­è·å–ä¸€ä¸ªè¿æ¥è¿›è¡Œå¥åº·æ£€æŸ¥
    select {
    case conn := <-pool.Connections:
        if err := conn.PingContext(ctx); err != nil {
            pool.HealthStatus = "unhealthy"
            conn.Close()
        } else {
            pool.HealthStatus = "healthy"
            pool.Connections <- conn
        }
    default:
        // è¿æ¥æ± ä¸ºç©ºï¼Œåˆ›å»ºä¸´æ—¶è¿æ¥æµ‹è¯•
        password, err := ds.decryptPassword(pool.DataSource.PasswordEncrypted)
        if err != nil {
            pool.HealthStatus = "error"
            return
        }
        
        dsn := ds.buildConnectionString(pool.DataSource, password)
        conn, err := sql.Open(pool.DataSource.DBType, dsn)
        if err != nil {
            pool.HealthStatus = "error"
            return
        }
        defer conn.Close()
        
        if err := conn.PingContext(ctx); err != nil {
            pool.HealthStatus = "unhealthy"
        } else {
            pool.HealthStatus = "healthy"
        }
    }
    
    // æ›´æ–°æ•°æ®åº“ä¸­çš„å¥åº·çŠ¶æ€
    ds.repo.UpdateHealthStatus(context.Background(), pool.DataSource.ID, pool.HealthStatus, "")
}
```

### 2. å…ƒæ•°æ®ç®¡ç†æœåŠ¡

```go
// internal/service/metadata_service.go
package service

import (
    "context"
    "database/sql"
    "fmt"
    "sort"
    "strings"
    "sync"
    "time"
)

type MetadataService struct {
    repo              repository.MetadataRepository
    dataSourceService *DataSourceService
    syncMutex         sync.Mutex
    syncStatus        map[string]*SyncStatus
}

type SyncStatus struct {
    DataSourceID string
    Status       string // pending, running, completed, failed
    StartTime    time.Time
    EndTime      time.Time
    Progress     int
    Error        string
    TablesFound  int
    TablesSync   int
}

type DatabaseIntrospector interface {
    GetSchemas(ctx context.Context, conn *sql.DB) ([]SchemaInfo, error)
    GetTables(ctx context.Context, conn *sql.DB, schemaName string) ([]TableInfo, error)
    GetColumns(ctx context.Context, conn *sql.DB, schemaName, tableName string) ([]ColumnInfo, error)
    GetForeignKeys(ctx context.Context, conn *sql.DB, schemaName, tableName string) ([]ForeignKeyInfo, error)
    GetIndexes(ctx context.Context, conn *sql.DB, schemaName, tableName string) ([]IndexInfo, error)
    GetTableStats(ctx context.Context, conn *sql.DB, schemaName, tableName string) (*TableStats, error)
}

type SchemaInfo struct {
    Name        string
    Description string
    TableCount  int
    TotalSize   int64
}

type TableInfo struct {
    Name        string
    Type        string // table, view, materialized_view
    Comment     string
    Owner       string
    CreatedAt   time.Time
    UpdatedAt   time.Time
    RowCount    int64
    SizeBytes   int64
}

type ColumnInfo struct {
    Name            string
    OrdinalPosition int
    DataType        string
    MaxLength       *int
    NumericPrecision *int
    NumericScale    *int
    IsNullable      bool
    ColumnDefault   *string
    Comment         string
    IsPrimaryKey    bool
    IsUnique        bool
    IsIndexed       bool
}

type ForeignKeyInfo struct {
    ConstraintName   string
    ColumnName       string
    RefTableSchema   string
    RefTableName     string
    RefColumnName    string
}

type IndexInfo struct {
    IndexName   string
    ColumnName  string
    IsUnique    bool
    OrdinalPosition int
}

type TableStats struct {
    RowCount    int64
    SizeBytes   int64
    LastUpdated time.Time
}

func NewMetadataService(repo repository.MetadataRepository, dataSourceService *DataSourceService) *MetadataService {
    return &MetadataService{
        repo:              repo,
        dataSourceService: dataSourceService,
        syncStatus:        make(map[string]*SyncStatus),
    }
}

// SyncMetadata åŒæ­¥æ•°æ®æºå…ƒæ•°æ®
func (ms *MetadataService) SyncMetadata(ctx context.Context, dataSourceID string, fullSync bool) error {
    ms.syncMutex.Lock()
    if status, exists := ms.syncStatus[dataSourceID]; exists && status.Status == "running" {
        ms.syncMutex.Unlock()
        return fmt.Errorf("sync already in progress for data source: %s", dataSourceID)
    }
    
    // åˆ›å»ºåŒæ­¥çŠ¶æ€
    status := &SyncStatus{
        DataSourceID: dataSourceID,
        Status:       "running",
        StartTime:    time.Now(),
        Progress:     0,
    }
    ms.syncStatus[dataSourceID] = status
    ms.syncMutex.Unlock()
    
    // å¼‚æ­¥æ‰§è¡ŒåŒæ­¥
    go func() {
        defer func() {
            ms.syncMutex.Lock()
            status.Status = "completed"
            status.EndTime = time.Now()
            ms.syncMutex.Unlock()
        }()
        
        if err := ms.performSync(ctx, dataSourceID, fullSync, status); err != nil {
            ms.syncMutex.Lock()
            status.Status = "failed"
            status.Error = err.Error()
            ms.syncMutex.Unlock()
        }
    }()
    
    return nil
}

// GetSyncStatus è·å–åŒæ­¥çŠ¶æ€
func (ms *MetadataService) GetSyncStatus(dataSourceID string) *SyncStatus {
    ms.syncMutex.Lock()
    defer ms.syncMutex.Unlock()
    
    status, exists := ms.syncStatus[dataSourceID]
    if !exists {
        return &SyncStatus{
            DataSourceID: dataSourceID,
            Status:       "never_synced",
        }
    }
    
    return status
}

// GetDatabaseSchema è·å–æ•°æ®åº“Schemaä¿¡æ¯ï¼ˆç”¨äºLLMä¸Šä¸‹æ–‡ï¼‰
func (ms *MetadataService) GetDatabaseSchema(ctx context.Context, dataSourceID string, schemaNames []string) (*DatabaseSchema, error) {
    // è·å–æŒ‡å®šæ•°æ®æºçš„å…ƒæ•°æ®
    schemas, err := ms.repo.GetSchemasByDataSource(ctx, dataSourceID)
    if err != nil {
        return nil, fmt.Errorf("get schemas: %w", err)
    }
    
    var filteredSchemas []*model.DatabaseSchema
    if len(schemaNames) > 0 {
        schemaMap := make(map[string]bool)
        for _, name := range schemaNames {
            schemaMap[name] = true
        }
        
        for _, schema := range schemas {
            if schemaMap[schema.SchemaName] {
                filteredSchemas = append(filteredSchemas, schema)
            }
        }
    } else {
        filteredSchemas = schemas
    }
    
    // æ„å»ºè¿”å›ç»“æœ
    dbSchema := &DatabaseSchema{
        Tables: make([]TableSchema, 0),
    }
    
    for _, schema := range filteredSchemas {
        tables, err := ms.repo.GetTablesBySchema(ctx, schema.ID)
        if err != nil {
            continue
        }
        
        for _, table := range tables {
            columns, err := ms.repo.GetColumnsByTable(ctx, table.ID)
            if err != nil {
                continue
            }
            
            tableSchema := TableSchema{
                Name:    table.TableName,
                Comment: table.BusinessDescription,
                Columns: make([]ColumnSchema, 0),
            }
            
            for _, column := range columns {
                columnSchema := ColumnSchema{
                    Name:         column.ColumnName,
                    Type:         column.DataType,
                    Comment:      column.BusinessDescription,
                    IsPrimaryKey: column.IsPrimaryKey,
                    IsForeignKey: column.IsForeignKey,
                    IsNullable:   column.IsNullable,
                }
                
                tableSchema.Columns = append(tableSchema.Columns, columnSchema)
            }
            
            dbSchema.Tables = append(dbSchema.Tables, tableSchema)
        }
    }
    
    return dbSchema, nil
}

// SearchTables æœç´¢è¡¨
func (ms *MetadataService) SearchTables(ctx context.Context, dataSourceID, keyword string, tags []string, limit int) ([]*model.TableMetadata, error) {
    return ms.repo.SearchTables(ctx, dataSourceID, keyword, tags, limit)
}

// GetTableDetails è·å–è¡¨è¯¦ç»†ä¿¡æ¯
func (ms *MetadataService) GetTableDetails(ctx context.Context, tableID string) (*model.TableDetails, error) {
    // è·å–è¡¨åŸºæœ¬ä¿¡æ¯
    table, err := ms.repo.GetTableByID(ctx, tableID)
    if err != nil {
        return nil, fmt.Errorf("get table: %w", err)
    }
    
    // è·å–å­—æ®µä¿¡æ¯
    columns, err := ms.repo.GetColumnsByTable(ctx, tableID)
    if err != nil {
        return nil, fmt.Errorf("get columns: %w", err)
    }
    
    // è·å–å…³ç³»ä¿¡æ¯
    relationships, err := ms.repo.GetTableRelationships(ctx, tableID)
    if err != nil {
        return nil, fmt.Errorf("get relationships: %w", err)
    }
    
    // è·å–æ•°æ®è´¨é‡ä¿¡æ¯
    qualityRules, err := ms.repo.GetDataQualityRules(ctx, tableID)
    if err != nil {
        return nil, fmt.Errorf("get quality rules: %w", err)
    }
    
    return &model.TableDetails{
        Table:         table,
        Columns:       columns,
        Relationships: relationships,
        QualityRules:  qualityRules,
    }, nil
}

// UpdateTableDescription æ›´æ–°è¡¨æè¿°
func (ms *MetadataService) UpdateTableDescription(ctx context.Context, tableID, description string, updatedBy string) error {
    return ms.repo.UpdateTableDescription(ctx, tableID, description, updatedBy)
}

// UpdateColumnDescription æ›´æ–°å­—æ®µæè¿°
func (ms *MetadataService) UpdateColumnDescription(ctx context.Context, columnID, description string, updatedBy string) error {
    return ms.repo.UpdateColumnDescription(ctx, columnID, description, updatedBy)
}

// AddTableTag æ·»åŠ è¡¨æ ‡ç­¾
func (ms *MetadataService) AddTableTag(ctx context.Context, tableID string, tags []string, addedBy string) error {
    return ms.repo.AddTableTags(ctx, tableID, tags, addedBy)
}

// ç§æœ‰æ–¹æ³•

func (ms *MetadataService) performSync(ctx context.Context, dataSourceID string, fullSync bool, status *SyncStatus) error {
    // 1. è·å–æ•°æ®æºä¿¡æ¯
    dataSource, err := ms.dataSourceService.repo.GetByID(ctx, dataSourceID)
    if err != nil {
        return fmt.Errorf("get data source: %w", err)
    }
    
    // 2. è·å–æ•°æ®åº“è¿æ¥
    conn, err := ms.dataSourceService.GetConnection(ctx, dataSourceID)
    if err != nil {
        return fmt.Errorf("get connection: %w", err)
    }
    defer ms.dataSourceService.ReleaseConnection(dataSourceID, conn)
    
    // 3. åˆ›å»ºæ•°æ®åº“å†…çœå™¨
    introspector := ms.createIntrospector(dataSource.DBType)
    if introspector == nil {
        return fmt.Errorf("unsupported database type: %s", dataSource.DBType)
    }
    
    // 4. è·å–Schemaåˆ—è¡¨
    schemas, err := introspector.GetSchemas(ctx, conn)
    if err != nil {
        return fmt.Errorf("get schemas: %w", err)
    }
    
    status.Progress = 10
    
    // 5. åŒæ­¥æ¯ä¸ªSchema
    totalTables := 0
    for _, schemaInfo := range schemas {
        totalTables += schemaInfo.TableCount
    }
    status.TablesFound = totalTables
    
    processedTables := 0
    for _, schemaInfo := range schemas {
        // åŒæ­¥Schemaä¿¡æ¯
        schema, err := ms.syncSchema(ctx, dataSourceID, schemaInfo)
        if err != nil {
            return fmt.Errorf("sync schema %s: %w", schemaInfo.Name, err)
        }
        
        // è·å–Schemaä¸‹çš„è¡¨
        tables, err := introspector.GetTables(ctx, conn, schemaInfo.Name)
        if err != nil {
            return fmt.Errorf("get tables for schema %s: %w", schemaInfo.Name, err)
        }
        
        // åŒæ­¥è¡¨ä¿¡æ¯
        for _, tableInfo := range tables {
            if err := ms.syncTable(ctx, conn, introspector, schema.ID, schemaInfo.Name, tableInfo); err != nil {
                return fmt.Errorf("sync table %s.%s: %w", schemaInfo.Name, tableInfo.Name, err)
            }
            
            processedTables++
            status.TablesSync = processedTables
            status.Progress = 10 + (processedTables*80)/totalTables
        }
    }
    
    // 6. åˆ†æè¡¨å…³ç³»
    if err := ms.analyzeRelationships(ctx, dataSourceID); err != nil {
        return fmt.Errorf("analyze relationships: %w", err)
    }
    
    status.Progress = 95
    
    // 7. æ¸…ç†è¿‡æ—¶çš„å…ƒæ•°æ®
    if fullSync {
        if err := ms.cleanupObsoleteMetadata(ctx, dataSourceID); err != nil {
            return fmt.Errorf("cleanup obsolete metadata: %w", err)
        }
    }
    
    status.Progress = 100
    
    return nil
}

func (ms *MetadataService) createIntrospector(dbType string) DatabaseIntrospector {
    switch dbType {
    case "mysql":
        return &MySQLIntrospector{}
    case "postgresql":
        return &PostgreSQLIntrospector{}
    case "sqlserver":
        return &SQLServerIntrospector{}
    case "oracle":
        return &OracleIntrospector{}
    default:
        return nil
    }
}

func (ms *MetadataService) syncSchema(ctx context.Context, dataSourceID string, schemaInfo SchemaInfo) (*model.DatabaseSchema, error) {
    // æ£€æŸ¥Schemaæ˜¯å¦å·²å­˜åœ¨
    existing, err := ms.repo.GetSchemaByName(ctx, dataSourceID, schemaInfo.Name)
    if err == nil {
        // æ›´æ–°ç°æœ‰Schema
        existing.Description = schemaInfo.Description
        existing.TableCount = schemaInfo.TableCount
        existing.TotalSizeBytes = schemaInfo.TotalSize
        existing.LastSyncedAt = time.Now()
        existing.SyncStatus = "completed"
        existing.UpdatedAt = time.Now()
        
        return existing, ms.repo.UpdateSchema(ctx, existing)
    }
    
    // åˆ›å»ºæ–°Schema
    schema := &model.DatabaseSchema{
        DataSourceID:   dataSourceID,
        SchemaName:     schemaInfo.Name,
        Description:    schemaInfo.Description,
        TableCount:     schemaInfo.TableCount,
        TotalSizeBytes: schemaInfo.TotalSize,
        LastSyncedAt:   time.Now(),
        SyncStatus:     "completed",
        CreatedAt:      time.Now(),
        UpdatedAt:      time.Now(),
    }
    
    return schema, ms.repo.CreateSchema(ctx, schema)
}

func (ms *MetadataService) syncTable(ctx context.Context, conn *sql.DB, introspector DatabaseIntrospector, schemaID, schemaName string, tableInfo TableInfo) error {
    // æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
    existing, err := ms.repo.GetTableByName(ctx, schemaID, tableInfo.Name)
    if err == nil {
        // æ›´æ–°ç°æœ‰è¡¨
        existing.TableType = tableInfo.Type
        existing.TechnicalDescription = tableInfo.Comment
        existing.Owner = tableInfo.Owner
        existing.RowCount = tableInfo.RowCount
        existing.SizeBytes = tableInfo.SizeBytes
        existing.CreatedAtDB = &tableInfo.CreatedAt
        existing.UpdatedAtDB = &tableInfo.UpdatedAt
        existing.UpdatedAt = time.Now()
        
        if err := ms.repo.UpdateTable(ctx, existing); err != nil {
            return err
        }
        
        // åŒæ­¥å­—æ®µä¿¡æ¯
        return ms.syncColumns(ctx, conn, introspector, existing.ID, schemaName, tableInfo.Name)
    }
    
    // åˆ›å»ºæ–°è¡¨
    table := &model.TableMetadata{
        SchemaID:             schemaID,
        TableName:            tableInfo.Name,
        TableType:            tableInfo.Type,
        DisplayName:          tableInfo.Name,
        TechnicalDescription: tableInfo.Comment,
        Owner:                tableInfo.Owner,
        RowCount:             tableInfo.RowCount,
        SizeBytes:            tableInfo.SizeBytes,
        CreatedAtDB:          &tableInfo.CreatedAt,
        UpdatedAtDB:          &tableInfo.UpdatedAt,
        CreatedAt:            time.Now(),
        UpdatedAt:            time.Now(),
    }
    
    if err := ms.repo.CreateTable(ctx, table); err != nil {
        return err
    }
    
    // åŒæ­¥å­—æ®µä¿¡æ¯
    return ms.syncColumns(ctx, conn, introspector, table.ID, schemaName, tableInfo.Name)
}

func (ms *MetadataService) syncColumns(ctx context.Context, conn *sql.DB, introspector DatabaseIntrospector, tableID, schemaName, tableName string) error {
    // è·å–å­—æ®µä¿¡æ¯
    columns, err := introspector.GetColumns(ctx, conn, schemaName, tableName)
    if err != nil {
        return fmt.Errorf("get columns: %w", err)
    }
    
    // è·å–å¤–é”®ä¿¡æ¯
    foreignKeys, err := introspector.GetForeignKeys(ctx, conn, schemaName, tableName)
    if err != nil {
        return fmt.Errorf("get foreign keys: %w", err)
    }
    
    // æ„å»ºå¤–é”®æ˜ å°„
    fkMap := make(map[string]*ForeignKeyInfo)
    for _, fk := range foreignKeys {
        fkMap[fk.ColumnName] = &fk
    }
    
    // è·å–ç´¢å¼•ä¿¡æ¯
    indexes, err := introspector.GetIndexes(ctx, conn, schemaName, tableName)
    if err != nil {
        return fmt.Errorf("get indexes: %w", err)
    }
    
    // æ„å»ºç´¢å¼•æ˜ å°„
    indexMap := make(map[string]bool)
    for _, idx := range indexes {
        indexMap[idx.ColumnName] = true
    }
    
    // åŒæ­¥æ¯ä¸ªå­—æ®µ
    for _, columnInfo := range columns {
        // æ£€æŸ¥å­—æ®µæ˜¯å¦å·²å­˜åœ¨
        existing, err := ms.repo.GetColumnByName(ctx, tableID, columnInfo.Name)
        if err == nil {
            // æ›´æ–°ç°æœ‰å­—æ®µ
            ms.updateExistingColumn(existing, columnInfo, fkMap, indexMap)
            if err := ms.repo.UpdateColumn(ctx, existing); err != nil {
                return err
            }
            continue
        }
        
        // åˆ›å»ºæ–°å­—æ®µ
        column := ms.createNewColumn(tableID, columnInfo, fkMap, indexMap)
        if err := ms.repo.CreateColumn(ctx, column); err != nil {
            return err
        }
    }
    
    return nil
}

func (ms *MetadataService) updateExistingColumn(existing *model.ColumnMetadata, columnInfo ColumnInfo, fkMap map[string]*ForeignKeyInfo, indexMap map[string]bool) {
    existing.OrdinalPosition = columnInfo.OrdinalPosition
    existing.DataType = columnInfo.DataType
    existing.MaxLength = columnInfo.MaxLength
    existing.NumericPrecision = columnInfo.NumericPrecision
    existing.NumericScale = columnInfo.NumericScale
    existing.IsNullable = columnInfo.IsNullable
    existing.ColumnDefault = columnInfo.ColumnDefault
    existing.IsPrimaryKey = columnInfo.IsPrimaryKey
    existing.IsUnique = columnInfo.IsUnique
    existing.IsIndexed = indexMap[columnInfo.Name]
    existing.UpdatedAt = time.Now()
    
    // æ›´æ–°å¤–é”®ä¿¡æ¯
    if fk, exists := fkMap[columnInfo.Name]; exists {
        existing.IsForeignKey = true
        existing.ForeignKeyTable = &fk.RefTableName
        existing.ForeignKeyColumn = &fk.RefColumnName
    } else {
        existing.IsForeignKey = false
        existing.ForeignKeyTable = nil
        existing.ForeignKeyColumn = nil
    }
}

func (ms *MetadataService) createNewColumn(tableID string, columnInfo ColumnInfo, fkMap map[string]*ForeignKeyInfo, indexMap map[string]bool) *model.ColumnMetadata {
    column := &model.ColumnMetadata{
        TableID:          tableID,
        ColumnName:       columnInfo.Name,
        OrdinalPosition:  columnInfo.OrdinalPosition,
        DataType:         columnInfo.DataType,
        MaxLength:        columnInfo.MaxLength,
        NumericPrecision: columnInfo.NumericPrecision,
        NumericScale:     columnInfo.NumericScale,
        IsNullable:       columnInfo.IsNullable,
        ColumnDefault:    columnInfo.ColumnDefault,
        IsPrimaryKey:     columnInfo.IsPrimaryKey,
        IsUnique:         columnInfo.IsUnique,
        IsIndexed:        indexMap[columnInfo.Name],
        DisplayName:      columnInfo.Name,
        CreatedAt:        time.Now(),
        UpdatedAt:        time.Now(),
    }
    
    // è®¾ç½®å¤–é”®ä¿¡æ¯
    if fk, exists := fkMap[columnInfo.Name]; exists {
        column.IsForeignKey = true
        column.ForeignKeyTable = &fk.RefTableName
        column.ForeignKeyColumn = &fk.RefColumnName
    }
    
    return column
}

func (ms *MetadataService) analyzeRelationships(ctx context.Context, dataSourceID string) error {
    // åŸºäºå¤–é”®çº¦æŸåˆ†æè¡¨å…³ç³»
    relationships, err := ms.repo.AnalyzeTableRelationships(ctx, dataSourceID)
    if err != nil {
        return fmt.Errorf("analyze relationships: %w", err)
    }
    
    // ä¿å­˜å…³ç³»ä¿¡æ¯
    for _, rel := range relationships {
        if err := ms.repo.CreateOrUpdateRelationship(ctx, rel); err != nil {
            return fmt.Errorf("save relationship: %w", err)
        }
    }
    
    return nil
}

func (ms *MetadataService) cleanupObsoleteMetadata(ctx context.Context, dataSourceID string) error {
    // æ¸…ç†ä¸å†å­˜åœ¨çš„è¡¨å’Œå­—æ®µ
    return ms.repo.CleanupObsoleteMetadata(ctx, dataSourceID, time.Now().Add(-1*time.Hour))
}

// å®šæœŸåŒæ­¥ä»»åŠ¡
func (ms *MetadataService) StartScheduledSync() {
    ticker := time.NewTicker(1 * time.Hour) // æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    go func() {
        for range ticker.C {
            ms.performScheduledSync()
        }
    }()
}

func (ms *MetadataService) performScheduledSync() {
    ctx := context.Background()
    
    // è·å–éœ€è¦åŒæ­¥çš„æ•°æ®æº
    dataSources, err := ms.dataSourceService.repo.ListActiveDataSources(ctx)
    if err != nil {
        log.Printf("Failed to list active data sources: %v", err)
        return
    }
    
    for _, dataSource := range dataSources {
        // æ£€æŸ¥ä¸Šæ¬¡åŒæ­¥æ—¶é—´
        lastSync, err := ms.repo.GetLastSyncTime(ctx, dataSource.ID)
        if err != nil {
            continue
        }
        
        // å¦‚æœè¶…è¿‡6å°æ—¶æœªåŒæ­¥ï¼Œè§¦å‘å¢é‡åŒæ­¥
        if time.Since(lastSync) > 6*time.Hour {
            ms.SyncMetadata(ctx, dataSource.ID, false)
        }
        
        // å¦‚æœè¶…è¿‡24å°æ—¶æœªå…¨é‡åŒæ­¥ï¼Œè§¦å‘å…¨é‡åŒæ­¥
        if time.Since(lastSync) > 24*time.Hour {
            ms.SyncMetadata(ctx, dataSource.ID, true)
        }
    }
}
```

### 3. MySQLæ•°æ®åº“å†…çœå™¨å®ç°

```go
// internal/service/mysql_introspector.go
package service

import (
    "context"
    "database/sql"
    "fmt"
    "strconv"
    "strings"
    "time"
)

type MySQLIntrospector struct{}

func (m *MySQLIntrospector) GetSchemas(ctx context.Context, conn *sql.DB) ([]SchemaInfo, error) {
    query := `
        SELECT 
            SCHEMA_NAME,
            DEFAULT_CHARACTER_SET_NAME as description,
            COUNT(*) as table_count
        FROM information_schema.SCHEMATA s
        LEFT JOIN information_schema.TABLES t ON s.SCHEMA_NAME = t.TABLE_SCHEMA
        WHERE SCHEMA_NAME NOT IN ('information_schema', 'performance_schema', 'mysql', 'sys')
        GROUP BY SCHEMA_NAME, DEFAULT_CHARACTER_SET_NAME
        ORDER BY SCHEMA_NAME
    `
    
    rows, err := conn.QueryContext(ctx, query)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var schemas []SchemaInfo
    for rows.Next() {
        var schema SchemaInfo
        var description sql.NullString
        var tableCount sql.NullInt64
        
        if err := rows.Scan(&schema.Name, &description, &tableCount); err != nil {
            return nil, err
        }
        
        schema.Description = description.String
        schema.TableCount = int(tableCount.Int64)
        
        schemas = append(schemas, schema)
    }
    
    return schemas, rows.Err()
}

func (m *MySQLIntrospector) GetTables(ctx context.Context, conn *sql.DB, schemaName string) ([]TableInfo, error) {
    query := `
        SELECT 
            TABLE_NAME,
            TABLE_TYPE,
            IFNULL(TABLE_COMMENT, '') as TABLE_COMMENT,
            IFNULL(TABLE_ROWS, 0) as row_count,
            IFNULL(DATA_LENGTH + INDEX_LENGTH, 0) as size_bytes,
            CREATE_TIME,
            UPDATE_TIME
        FROM information_schema.TABLES
        WHERE TABLE_SCHEMA = ? 
        AND TABLE_TYPE IN ('BASE TABLE', 'VIEW')
        ORDER BY TABLE_NAME
    `
    
    rows, err := conn.QueryContext(ctx, query, schemaName)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var tables []TableInfo
    for rows.Next() {
        var table TableInfo
        var tableType string
        var createTime, updateTime sql.NullTime
        
        if err := rows.Scan(
            &table.Name,
            &tableType,
            &table.Comment,
            &table.RowCount,
            &table.SizeBytes,
            &createTime,
            &updateTime,
        ); err != nil {
            return nil, err
        }
        
        // è½¬æ¢è¡¨ç±»å‹
        switch tableType {
        case "BASE TABLE":
            table.Type = "table"
        case "VIEW":
            table.Type = "view"
        default:
            table.Type = strings.ToLower(tableType)
        }
        
        if createTime.Valid {
            table.CreatedAt = createTime.Time
        }
        if updateTime.Valid {
            table.UpdatedAt = updateTime.Time
        }
        
        tables = append(tables, table)
    }
    
    return tables, rows.Err()
}

func (m *MySQLIntrospector) GetColumns(ctx context.Context, conn *sql.DB, schemaName, tableName string) ([]ColumnInfo, error) {
    query := `
        SELECT 
            COLUMN_NAME,
            ORDINAL_POSITION,
            DATA_TYPE,
            CHARACTER_MAXIMUM_LENGTH,
            NUMERIC_PRECISION,
            NUMERIC_SCALE,
            IF(IS_NULLABLE = 'YES', 1, 0) as is_nullable,
            COLUMN_DEFAULT,
            IFNULL(COLUMN_COMMENT, '') as column_comment,
            IF(COLUMN_KEY = 'PRI', 1, 0) as is_primary_key,
            IF(COLUMN_KEY = 'UNI', 1, 0) as is_unique,
            IF(COLUMN_KEY IN ('PRI', 'UNI', 'MUL'), 1, 0) as is_indexed
        FROM information_schema.COLUMNS
        WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?
        ORDER BY ORDINAL_POSITION
    `
    
    rows, err := conn.QueryContext(ctx, query, schemaName, tableName)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var columns []ColumnInfo
    for rows.Next() {
        var column ColumnInfo
        var maxLength, precision, scale sql.NullInt64
        var columnDefault sql.NullString
        
        if err := rows.Scan(
            &column.Name,
            &column.OrdinalPosition,
            &column.DataType,
            &maxLength,
            &precision,
            &scale,
            &column.IsNullable,
            &columnDefault,
            &column.Comment,
            &column.IsPrimaryKey,
            &column.IsUnique,
            &column.IsIndexed,
        ); err != nil {
            return nil, err
        }
        
        if maxLength.Valid {
            length := int(maxLength.Int64)
            column.MaxLength = &length
        }
        if precision.Valid {
            prec := int(precision.Int64)
            column.NumericPrecision = &prec
        }
        if scale.Valid {
            sc := int(scale.Int64)
            column.NumericScale = &sc
        }
        if columnDefault.Valid {
            column.ColumnDefault = &columnDefault.String
        }
        
        columns = append(columns, column)
    }
    
    return columns, rows.Err()
}

func (m *MySQLIntrospector) GetForeignKeys(ctx context.Context, conn *sql.DB, schemaName, tableName string) ([]ForeignKeyInfo, error) {
    query := `
        SELECT 
            CONSTRAINT_NAME,
            COLUMN_NAME,
            REFERENCED_TABLE_SCHEMA,
            REFERENCED_TABLE_NAME,
            REFERENCED_COLUMN_NAME
        FROM information_schema.KEY_COLUMN_USAGE
        WHERE TABLE_SCHEMA = ? 
        AND TABLE_NAME = ?
        AND REFERENCED_TABLE_NAME IS NOT NULL
        ORDER BY ORDINAL_POSITION
    `
    
    rows, err := conn.QueryContext(ctx, query, schemaName, tableName)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var foreignKeys []ForeignKeyInfo
    for rows.Next() {
        var fk ForeignKeyInfo
        
        if err := rows.Scan(
            &fk.ConstraintName,
            &fk.ColumnName,
            &fk.RefTableSchema,
            &fk.RefTableName,
            &fk.RefColumnName,
        ); err != nil {
            return nil, err
        }
        
        foreignKeys = append(foreignKeys, fk)
    }
    
    return foreignKeys, rows.Err()
}

func (m *MySQLIntrospector) GetIndexes(ctx context.Context, conn *sql.DB, schemaName, tableName string) ([]IndexInfo, error) {
    query := `
        SELECT 
            INDEX_NAME,
            COLUMN_NAME,
            IF(NON_UNIQUE = 0, 1, 0) as is_unique,
            SEQ_IN_INDEX as ordinal_position
        FROM information_schema.STATISTICS
        WHERE TABLE_SCHEMA = ? 
        AND TABLE_NAME = ?
        ORDER BY INDEX_NAME, SEQ_IN_INDEX
    `
    
    rows, err := conn.QueryContext(ctx, query, schemaName, tableName)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    var indexes []IndexInfo
    for rows.Next() {
        var index IndexInfo
        
        if err := rows.Scan(
            &index.IndexName,
            &index.ColumnName,
            &index.IsUnique,
            &index.OrdinalPosition,
        ); err != nil {
            return nil, err
        }
        
        indexes = append(indexes, index)
    }
    
    return indexes, rows.Err()
}

func (m *MySQLIntrospector) GetTableStats(ctx context.Context, conn *sql.DB, schemaName, tableName string) (*TableStats, error) {
    query := `
        SELECT 
            IFNULL(TABLE_ROWS, 0) as row_count,
            IFNULL(DATA_LENGTH + INDEX_LENGTH, 0) as size_bytes,
            IFNULL(UPDATE_TIME, CREATE_TIME) as last_updated
        FROM information_schema.TABLES
        WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?
    `
    
    var stats TableStats
    var lastUpdated sql.NullTime
    
    err := conn.QueryRowContext(ctx, query, schemaName, tableName).Scan(
        &stats.RowCount,
        &stats.SizeBytes,
        &lastUpdated,
    )
    
    if err != nil {
        return nil, err
    }
    
    if lastUpdated.Valid {
        stats.LastUpdated = lastUpdated.Time
    }
    
    return &stats, nil
}
```

### 4. æŸ¥è¯¢æ‰§è¡Œå¼•æ“

```go
// internal/service/query_engine.go
package service

import (
    "context"
    "crypto/sha256"
    "database/sql"
    "encoding/hex"
    "fmt"
    "regexp"
    "strings"
    "sync"
    "time"
)

type QueryEngine struct {
    dataSourceService *DataSourceService
    metadataService   *MetadataService
    permissionService *service.PermissionService
    executionRepo     repository.QueryExecutionRepository
    cache             map[string]*CachedResult
    cacheMutex        sync.RWMutex
    cacheTTL          time.Duration
}

type QueryRequest struct {
    DataSourceID string `json:"data_source_id" validate:"required"`
    SQL          string `json:"sql" validate:"required"`
    UserID       string `json:"user_id" validate:"required"`
    SessionID    string `json:"session_id,omitempty"`
    MaxRows      int    `json:"max_rows,omitempty"`
    Timeout      int    `json:"timeout,omitempty"`
}

type QueryResult struct {
    ExecutionID    string                   `json:"execution_id"`
    Data           []map[string]interface{} `json:"data"`
    Columns        []ColumnInfo             `json:"columns"`
    RowCount       int                      `json:"row_count"`
    ExecutionTime  time.Duration            `json:"execution_time"`
    FromCache      bool                     `json:"from_cache"`
    BytesScanned   int64                    `json:"bytes_scanned"`
    QueryPlan      interface{}              `json:"query_plan,omitempty"`
}

type CachedResult struct {
    Result    *QueryResult
    ExpiresAt time.Time
    HitCount  int64
}

type QueryValidator struct {
    allowedStatements []string
    forbiddenKeywords []string
    maxQueryLength    int
}

func NewQueryEngine(
    dataSourceService *DataSourceService,
    metadataService *MetadataService,
    permissionService *service.PermissionService,
    executionRepo repository.QueryExecutionRepository,
) *QueryEngine {
    return &QueryEngine{
        dataSourceService: dataSourceService,
        metadataService:   metadataService,
        permissionService: permissionService,
        executionRepo:     executionRepo,
        cache:             make(map[string]*CachedResult),
        cacheTTL:          24 * time.Hour,
    }
}

// ExecuteQuery æ‰§è¡ŒæŸ¥è¯¢
func (qe *QueryEngine) ExecuteQuery(ctx context.Context, req *QueryRequest) (*QueryResult, error) {
    // 1. éªŒè¯æŸ¥è¯¢æƒé™
    if err := qe.validatePermissions(ctx, req); err != nil {
        return nil, fmt.Errorf("permission check failed: %w", err)
    }
    
    // 2. éªŒè¯SQLè¯­å¥
    if err := qe.validateSQL(req.SQL); err != nil {
        return nil, fmt.Errorf("SQL validation failed: %w", err)
    }
    
    // 3. æ£€æŸ¥ç¼“å­˜
    cacheKey := qe.generateCacheKey(req)
    if cached := qe.getFromCache(cacheKey); cached != nil {
        qe.updateCacheStats(cacheKey)
        return cached, nil
    }
    
    // 4. åˆ›å»ºæ‰§è¡Œè®°å½•
    execution := &model.QueryExecution{
        UserID:        req.UserID,
        SessionID:     req.SessionID,
        DataSourceID:  req.DataSourceID,
        SQLQuery:      req.SQL,
        SQLHash:       qe.hashSQL(req.SQL),
        QueryType:     qe.getQueryType(req.SQL),
        Status:        "pending",
        StartTime:     time.Now(),
    }
    
    if err := qe.executionRepo.Create(ctx, execution); err != nil {
        return nil, fmt.Errorf("create execution record: %w", err)
    }
    
    // 5. æ‰§è¡ŒæŸ¥è¯¢
    result, err := qe.doExecuteQuery(ctx, req, execution)
    if err != nil {
        // æ›´æ–°æ‰§è¡ŒçŠ¶æ€ä¸ºå¤±è´¥
        execution.Status = "failed"
        execution.EndTime = time.Now()
        execution.ErrorMessage = err.Error()
        qe.executionRepo.Update(ctx, execution)
        return nil, err
    }
    
    // 6. æ›´æ–°æ‰§è¡ŒçŠ¶æ€ä¸ºæˆåŠŸ
    execution.Status = "completed"
    execution.EndTime = time.Now()
    execution.ExecutionTimeMs = int(result.ExecutionTime.Milliseconds())
    execution.RowsReturned = result.RowCount
    qe.executionRepo.Update(ctx, execution)
    
    // 7. ç¼“å­˜ç»“æœ
    qe.putToCache(cacheKey, result)
    
    result.ExecutionID = execution.ID
    return result, nil
}

// GetQueryHistory è·å–æŸ¥è¯¢å†å²
func (qe *QueryEngine) GetQueryHistory(ctx context.Context, userID string, limit int) ([]*model.QueryExecution, error) {
    return qe.executionRepo.GetByUserID(ctx, userID, limit)
}

// GetQueryStats è·å–æŸ¥è¯¢ç»Ÿè®¡
func (qe *QueryEngine) GetQueryStats(ctx context.Context, userID string, dataSourceID string, timeRange string) (*QueryStats, error) {
    return qe.executionRepo.GetStats(ctx, userID, dataSourceID, timeRange)
}

// CancelQuery å–æ¶ˆæŸ¥è¯¢
func (qe *QueryEngine) CancelQuery(ctx context.Context, executionID string, userID string) error {
    execution, err := qe.executionRepo.GetByID(ctx, executionID)
    if err != nil {
        return fmt.Errorf("get execution: %w", err)
    }
    
    if execution.UserID != userID {
        return fmt.Errorf("permission denied")
    }
    
    if execution.Status != "running" {
        return fmt.Errorf("query is not running")
    }
    
    // æ›´æ–°çŠ¶æ€ä¸ºå–æ¶ˆ
    execution.Status = "cancelled"
    execution.EndTime = time.Now()
    
    return qe.executionRepo.Update(ctx, execution)
}

// ç§æœ‰æ–¹æ³•

func (qe *QueryEngine) validatePermissions(ctx context.Context, req *QueryRequest) error {
    // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æ•°æ®æºæŸ¥è¯¢æƒé™
    hasPermission, err := qe.permissionService.CheckPermission(
        ctx, req.UserID, "query", "execute")
    if err != nil {
        return err
    }
    
    if !hasPermission {
        return fmt.Errorf("insufficient permissions to execute queries")
    }
    
    // æ£€æŸ¥æ•°æ®æºçº§åˆ«æƒé™
    return qe.checkDataSourcePermission(ctx, req.UserID, req.DataSourceID)
}

func (qe *QueryEngine) checkDataSourcePermission(ctx context.Context, userID, dataSourceID string) error {
    // è¿™é‡Œå¯ä»¥å®ç°æ›´ç²¾ç»†çš„æ•°æ®æºæƒé™æ§åˆ¶
    // ä¾‹å¦‚ï¼šç”¨æˆ·åªèƒ½æŸ¥è¯¢ç‰¹å®šçš„è¡¨ã€åªèƒ½åœ¨ç‰¹å®šæ—¶é—´æ®µæŸ¥è¯¢ç­‰
    return nil
}

func (qe *QueryEngine) validateSQL(sql string) error {
    validator := &QueryValidator{
        allowedStatements: []string{"SELECT", "WITH"},
        forbiddenKeywords: []string{
            "DELETE", "DROP", "TRUNCATE", "INSERT", "UPDATE",
            "CREATE", "ALTER", "GRANT", "REVOKE", "EXEC",
        },
        maxQueryLength: 10000,
    }
    
    return validator.Validate(sql)
}

func (qv *QueryValidator) Validate(sql string) error {
    // 1. æ£€æŸ¥é•¿åº¦
    if len(sql) > qv.maxQueryLength {
        return fmt.Errorf("query too long (max %d characters)", qv.maxQueryLength)
    }
    
    // 2. ç§»é™¤æ³¨é‡Šå’Œå¤šä½™ç©ºæ ¼
    cleanSQL := qv.cleanSQL(sql)
    
    // 3. æ£€æŸ¥æ˜¯å¦ä»¥å…è®¸çš„è¯­å¥å¼€å¤´
    if !qv.startsWithAllowedStatement(cleanSQL) {
        return fmt.Errorf("only SELECT and WITH statements are allowed")
    }
    
    // 4. æ£€æŸ¥ç¦ç”¨å…³é”®è¯
    if qv.containsForbiddenKeywords(cleanSQL) {
        return fmt.Errorf("query contains forbidden keywords")
    }
    
    // 5. æ£€æŸ¥æ½œåœ¨çš„SQLæ³¨å…¥
    if qv.hasSQLInjectionRisk(cleanSQL) {
        return fmt.Errorf("query has potential SQL injection risk")
    }
    
    return nil
}

func (qv *QueryValidator) cleanSQL(sql string) string {
    // ç§»é™¤SQLæ³¨é‡Š
    sql = regexp.MustCompile(`--.*$`).ReplaceAllString(sql, "")
    sql = regexp.MustCompile(`/\*.*?\*/`).ReplaceAllString(sql, "")
    
    // ç§»é™¤å¤šä½™ç©ºæ ¼
    sql = regexp.MustCompile(`\s+`).ReplaceAllString(sql, " ")
    
    return strings.TrimSpace(strings.ToUpper(sql))
}

func (qv *QueryValidator) startsWithAllowedStatement(sql string) bool {
    for _, stmt := range qv.allowedStatements {
        if strings.HasPrefix(sql, stmt+" ") || strings.HasPrefix(sql, stmt+"(") {
            return true
        }
    }
    return false
}

func (qv *QueryValidator) containsForbiddenKeywords(sql string) bool {
    for _, keyword := range qv.forbiddenKeywords {
        if strings.Contains(sql, " "+keyword+" ") || 
           strings.Contains(sql, " "+keyword+"(") ||
           strings.HasSuffix(sql, " "+keyword) {
            return true
        }
    }
    return false
}

func (qv *QueryValidator) hasSQLInjectionRisk(sql string) bool {
    // æ£€æŸ¥ä¸€äº›å¸¸è§çš„SQLæ³¨å…¥æ¨¡å¼
    patterns := []string{
        `'; *(DROP|DELETE|UPDATE|INSERT)`,
        `UNION\s+SELECT`,
        `OR\s+1\s*=\s*1`,
        `AND\s+1\s*=\s*1`,
        `';--`,
        `'OR'`,
    }
    
    for _, pattern := range patterns {
        matched, _ := regexp.MatchString(pattern, sql)
        if matched {
            return true
        }
    }
    
    return false
}

func (qe *QueryEngine) doExecuteQuery(ctx context.Context, req *QueryRequest, execution *model.QueryExecution) (*QueryResult, error) {
    // æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
    execution.Status = "running"
    qe.executionRepo.Update(ctx, execution)
    
    // è·å–æ•°æ®åº“è¿æ¥
    conn, err := qe.dataSourceService.GetConnection(ctx, req.DataSourceID)
    if err != nil {
        return nil, fmt.Errorf("get connection: %w", err)
    }
    defer qe.dataSourceService.ReleaseConnection(req.DataSourceID, conn)
    
    // è®¾ç½®æŸ¥è¯¢è¶…æ—¶
    timeout := time.Duration(req.Timeout) * time.Second
    if timeout == 0 {
        timeout = 5 * time.Minute // é»˜è®¤5åˆ†é’Ÿè¶…æ—¶
    }
    
    queryCtx, cancel := context.WithTimeout(ctx, timeout)
    defer cancel()
    
    // åº”ç”¨è¡Œæ•°é™åˆ¶
    maxRows := req.MaxRows
    if maxRows == 0 || maxRows > 10000 {
        maxRows = 10000 // é»˜è®¤æœ€å¤§10000è¡Œ
    }
    
    // åœ¨SQLåæ·»åŠ LIMITå­å¥ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
    limitedSQL := qe.addLimitClause(req.SQL, maxRows)
    
    startTime := time.Now()
    
    // æ‰§è¡ŒæŸ¥è¯¢
    rows, err := conn.QueryContext(queryCtx, limitedSQL)
    if err != nil {
        return nil, fmt.Errorf("execute query: %w", err)
    }
    defer rows.Close()
    
    // è·å–åˆ—ä¿¡æ¯
    columns, err := rows.Columns()
    if err != nil {
        return nil, fmt.Errorf("get columns: %w", err)
    }
    
    columnTypes, err := rows.ColumnTypes()
    if err != nil {
        return nil, fmt.Errorf("get column types: %w", err)
    }
    
    // æ„å»ºåˆ—ä¿¡æ¯
    columnInfos := make([]ColumnInfo, len(columns))
    for i, col := range columns {
        columnInfos[i] = ColumnInfo{
            Name:     col,
            DataType: columnTypes[i].DatabaseTypeName(),
        }
    }
    
    // è¯»å–æ•°æ®
    var data []map[string]interface{}
    rowCount := 0
    
    for rows.Next() && rowCount < maxRows {
        // åˆ›å»ºæ¥æ”¶æ•°æ®çš„åˆ‡ç‰‡
        values := make([]interface{}, len(columns))
        valuePtrs := make([]interface{}, len(columns))
        for i := range values {
            valuePtrs[i] = &values[i]
        }
        
        if err := rows.Scan(valuePtrs...); err != nil {
            return nil, fmt.Errorf("scan row: %w", err)
        }
        
        // æ„å»ºè¡Œæ•°æ®
        row := make(map[string]interface{})
        for i, col := range columns {
            val := values[i]
            if b, ok := val.([]byte); ok {
                val = string(b)
            }
            row[col] = val
        }
        
        data = append(data, row)
        rowCount++
    }
    
    if err := rows.Err(); err != nil {
        return nil, fmt.Errorf("row iteration error: %w", err)
    }
    
    executionTime := time.Since(startTime)
    
    return &QueryResult{
        Data:          data,
        Columns:       columnInfos,
        RowCount:      rowCount,
        ExecutionTime: executionTime,
        FromCache:     false,
        BytesScanned:  int64(rowCount * len(columns) * 100), // ç²—ç•¥ä¼°ç®—
    }, nil
}

func (qe *QueryEngine) addLimitClause(sql string, maxRows int) string {
    upperSQL := strings.ToUpper(strings.TrimSpace(sql))
    
    // å¦‚æœå·²ç»æœ‰LIMITå­å¥ï¼Œä¸æ·»åŠ 
    if strings.Contains(upperSQL, " LIMIT ") {
        return sql
    }
    
    return fmt.Sprintf("%s LIMIT %d", sql, maxRows)
}

func (qe *QueryEngine) generateCacheKey(req *QueryRequest) string {
    data := fmt.Sprintf("%s:%s:%s", req.DataSourceID, req.UserID, req.SQL)
    hash := sha256.Sum256([]byte(data))
    return hex.EncodeToString(hash[:])
}

func (qe *QueryEngine) hashSQL(sql string) string {
    hash := sha256.Sum256([]byte(sql))
    return hex.EncodeToString(hash[:])
}

func (qe *QueryEngine) getQueryType(sql string) string {
    sql = strings.TrimSpace(strings.ToUpper(sql))
    
    switch {
    case strings.HasPrefix(sql, "SELECT"):
        return "SELECT"
    case strings.HasPrefix(sql, "WITH"):
        return "WITH"
    case strings.HasPrefix(sql, "SHOW"):
        return "SHOW"
    case strings.HasPrefix(sql, "DESCRIBE") || strings.HasPrefix(sql, "DESC"):
        return "DESCRIBE"
    default:
        return "OTHER"
    }
}

func (qe *QueryEngine) getFromCache(key string) *QueryResult {
    qe.cacheMutex.RLock()
    defer qe.cacheMutex.RUnlock()
    
    cached, exists := qe.cache[key]
    if !exists || time.Now().After(cached.ExpiresAt) {
        return nil
    }
    
    // å¤åˆ¶ç»“æœï¼Œæ ‡è®°ä¸ºæ¥è‡ªç¼“å­˜
    result := *cached.Result
    result.FromCache = true
    
    return &result
}

func (qe *QueryEngine) putToCache(key string, result *QueryResult) {
    qe.cacheMutex.Lock()
    defer qe.cacheMutex.Unlock()
    
    qe.cache[key] = &CachedResult{
        Result:    result,
        ExpiresAt: time.Now().Add(qe.cacheTTL),
        HitCount:  0,
    }
}

func (qe *QueryEngine) updateCacheStats(key string) {
    qe.cacheMutex.Lock()
    defer qe.cacheMutex.Unlock()
    
    if cached, exists := qe.cache[key]; exists {
        cached.HitCount++
    }
}

// å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
func (qe *QueryEngine) StartCacheCleanup() {
    ticker := time.NewTicker(1 * time.Hour)
    go func() {
        for range ticker.C {
            qe.cleanupExpiredCache()
        }
    }()
}

func (qe *QueryEngine) cleanupExpiredCache() {
    qe.cacheMutex.Lock()
    defer qe.cacheMutex.Unlock()
    
    now := time.Now()
    for key, cached := range qe.cache {
        if now.After(cached.ExpiresAt) {
            delete(qe.cache, key)
        }
    }
}
```

### 5. å‰ç«¯æ•°æ®æºç®¡ç†ç•Œé¢

```svelte
<!-- web/src/routes/admin/datasources/+page.svelte -->
<script lang="ts">
  import { onMount } from 'svelte';
  import { authStore } from '$lib/stores/authStore';
  import DataSourceCard from '$lib/components/admin/DataSourceCard.svelte';
  import DataSourceForm from '$lib/components/admin/DataSourceForm.svelte';
  import LoadingSpinner from '$lib/components/LoadingSpinner.svelte';
  import ProtectedRoute from '$lib/components/auth/ProtectedRoute.svelte';

  let dataSources: DataSource[] = [];
  let isLoading = true;
  let error = '';
  let showAddForm = false;
  let selectedDataSource: DataSource | null = null;

  interface DataSource {
    id: string;
    name: string;
    display_name: string;
    description: string;
    db_type: string;
    host: string;
    port: number;
    database_name: string;
    username: string;
    status: number;
    last_health_check?: string;
    created_at: string;
  }

  onMount(() => {
    loadDataSources();
  });

  async function loadDataSources() {
    try {
      isLoading = true;
      const response = await fetch('/api/v1/datasources', {
        headers: {
          'Authorization': `Bearer ${$authStore.token}`
        }
      });

      const result = await response.json();
      if (result.success) {
        dataSources = result.data;
      } else {
        error = result.message;
      }
    } catch (err) {
      error = 'åŠ è½½æ•°æ®æºå¤±è´¥';
      console.error('Load datasources error:', err);
    } finally {
      isLoading = false;
    }
  }

  async function handleTestConnection(dataSourceId: string) {
    try {
      const response = await fetch(`/api/v1/datasources/${dataSourceId}/test`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${$authStore.token}`
        }
      });

      const result = await response.json();
      if (result.success) {
        alert('è¿æ¥æµ‹è¯•æˆåŠŸï¼');
      } else {
        alert(`è¿æ¥æµ‹è¯•å¤±è´¥ï¼š${result.message}`);
      }
    } catch (err) {
      alert('è¿æ¥æµ‹è¯•å¤±è´¥');
    }
  }

  async function handleSyncMetadata(dataSourceId: string) {
    try {
      const response = await fetch(`/api/v1/datasources/${dataSourceId}/sync`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${$authStore.token}`
        }
      });

      const result = await response.json();
      if (result.success) {
        alert('å…ƒæ•°æ®åŒæ­¥å·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥çœ‹åŒæ­¥çŠ¶æ€');
      } else {
        alert(`åŒæ­¥å¤±è´¥ï¼š${result.message}`);
      }
    } catch (err) {
      alert('åŒæ­¥å¤±è´¥');
    }
  }

  function handleAddDataSource() {
    selectedDataSource = null;
    showAddForm = true;
  }

  function handleEditDataSource(dataSource: DataSource) {
    selectedDataSource = dataSource;
    showAddForm = true;
  }

  async function handleDeleteDataSource(dataSourceId: string) {
    if (!confirm('ç¡®å®šè¦åˆ é™¤è¿™ä¸ªæ•°æ®æºå—ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤ã€‚')) {
      return;
    }

    try {
      const response = await fetch(`/api/v1/datasources/${dataSourceId}`, {
        method: 'DELETE',
        headers: {
          'Authorization': `Bearer ${$authStore.token}`
        }
      });

      const result = await response.json();
      if (result.success) {
        await loadDataSources();
        alert('æ•°æ®æºåˆ é™¤æˆåŠŸ');
      } else {
        alert(`åˆ é™¤å¤±è´¥ï¼š${result.message}`);
      }
    } catch (err) {
      alert('åˆ é™¤å¤±è´¥');
    }
  }

  function handleFormSubmit() {
    showAddForm = false;
    loadDataSources();
  }

  function handleFormCancel() {
    showAddForm = false;
    selectedDataSource = null;
  }
</script>

<ProtectedRoute requiredPermission="datasource:read">
  <div class="container mx-auto p-6">
    <!-- é¡µé¢æ ‡é¢˜ -->
    <div class="flex justify-between items-center mb-6">
      <div>
        <h1 class="text-3xl font-bold">æ•°æ®æºç®¡ç†</h1>
        <p class="text-base-content/70 mt-1">
          ç®¡ç†å’Œé…ç½®ç³»ç»Ÿä¸­çš„æ•°æ®æºè¿æ¥
        </p>
      </div>
      
      {#if $authStore.hasPermission('datasource', 'create')}
        <button 
          class="btn btn-primary"
          on:click={handleAddDataSource}
        >
          <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 4v16m8-8H4" />
          </svg>
          æ·»åŠ æ•°æ®æº
        </button>
      {/if}
    </div>

    <!-- é”™è¯¯æç¤º -->
    {#if error}
      <div class="alert alert-error mb-6">
        <svg xmlns="http://www.w3.org/2000/svg" class="stroke-current shrink-0 h-6 w-6" fill="none" viewBox="0 0 24 24">
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 14l2-2m0 0l2-2m-2 2l-2-2m2 2l2 2m7-2a9 9 0 11-18 0 9 9 0 0118 0z" />
        </svg>
        <span>{error}</span>
      </div>
    {/if}

    <!-- æ•°æ®æºåˆ—è¡¨ -->
    {#if isLoading}
      <div class="flex justify-center py-12">
        <LoadingSpinner size="lg" />
        <span class="ml-4 text-lg">åŠ è½½æ•°æ®æºä¸­...</span>
      </div>
    {:else if dataSources.length === 0}
      <div class="text-center py-12">
        <div class="text-6xl mb-4">ğŸ—„ï¸</div>
        <h2 class="text-2xl font-semibold mb-2">æš‚æ— æ•°æ®æº</h2>
        <p class="text-base-content/70 mb-6">
          å¼€å§‹æ·»åŠ æ‚¨çš„ç¬¬ä¸€ä¸ªæ•°æ®æºä»¥ä½¿ç”¨æ™ºèƒ½æŸ¥è¯¢åŠŸèƒ½
        </p>
        <button 
          class="btn btn-primary"
          on:click={handleAddDataSource}
        >
          æ·»åŠ æ•°æ®æº
        </button>
      </div>
    {:else}
      <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
        {#each dataSources as dataSource (dataSource.id)}
          <DataSourceCard
            {dataSource}
            on:test={() => handleTestConnection(dataSource.id)}
            on:sync={() => handleSyncMetadata(dataSource.id)}
            on:edit={() => handleEditDataSource(dataSource)}
            on:delete={() => handleDeleteDataSource(dataSource.id)}
          />
        {/each}
      </div>
    {/if}

    <!-- æ·»åŠ /ç¼–è¾‘è¡¨å•æ¨¡æ€æ¡† -->
    {#if showAddForm}
      <div class="modal modal-open">
        <div class="modal-box w-11/12 max-w-2xl">
          <DataSourceForm
            dataSource={selectedDataSource}
            on:submit={handleFormSubmit}
            on:cancel={handleFormCancel}
          />
        </div>
      </div>
    {/if}
  </div>
</ProtectedRoute>
```

```svelte
<!-- web/src/lib/components/admin/DataSourceCard.svelte -->
<script lang="ts">
  import { createEventDispatcher } from 'svelte';
  import { authStore } from '$lib/stores/authStore';

  export let dataSource: any;

  const dispatch = createEventDispatcher();

  function getStatusColor(status: number) {
    switch (status) {
      case 1: return 'success';
      case 0: return 'warning';
      case -1: return 'error';
      default: return 'neutral';
    }
  }

  function getStatusText(status: number) {
    switch (status) {
      case 1: return 'æ­£å¸¸';
      case 0: return 'æœªæ¿€æ´»';
      case -1: return 'é”™è¯¯';
      default: return 'æœªçŸ¥';
    }
  }

  function getDbTypeIcon(dbType: string) {
    switch (dbType.toLowerCase()) {
      case 'mysql': return 'ğŸ¬';
      case 'postgresql': return 'ğŸ˜';
      case 'oracle': return 'ğŸ”¶';
      case 'sqlserver': return 'ğŸªŸ';
      case 'clickhouse': return 'âš¡';
      case 'mongodb': return 'ğŸƒ';
      default: return 'ğŸ—„ï¸';
    }
  }

  function formatDateTime(dateString: string) {
    return new Date(dateString).toLocaleString('zh-CN');
  }
</script>

<div class="card bg-base-100 shadow-xl">
  <div class="card-body">
    <!-- æ•°æ®æºæ ‡é¢˜ -->
    <div class="flex items-start justify-between mb-4">
      <div class="flex items-center">
        <span class="text-2xl mr-3">{getDbTypeIcon(dataSource.db_type)}</span>
        <div>
          <h2 class="card-title text-lg">{dataSource.display_name}</h2>
          <p class="text-sm text-base-content/70">{dataSource.name}</p>
        </div>
      </div>
      
      <div class="badge badge-{getStatusColor(dataSource.status)} badge-sm">
        {getStatusText(dataSource.status)}
      </div>
    </div>

    <!-- è¿æ¥ä¿¡æ¯ -->
    <div class="space-y-2 mb-4">
      <div class="flex justify-between text-sm">
        <span class="text-base-content/70">æ•°æ®åº“ç±»å‹:</span>
        <span class="font-medium uppercase">{dataSource.db_type}</span>
      </div>
      <div class="flex justify-between text-sm">
        <span class="text-base-content/70">ä¸»æœºåœ°å€:</span>
        <span class="font-mono">{dataSource.host}:{dataSource.port}</span>
      </div>
      <div class="flex justify-between text-sm">
        <span class="text-base-content/70">æ•°æ®åº“:</span>
        <span class="font-mono">{dataSource.database_name}</span>
      </div>
      <div class="flex justify-between text-sm">
        <span class="text-base-content/70">ç”¨æˆ·å:</span>
        <span class="font-mono">{dataSource.username}</span>
      </div>
    </div>

    <!-- æè¿° -->
    {#if dataSource.description}
      <p class="text-sm text-base-content/70 mb-4 line-clamp-2">
        {dataSource.description}
      </p>
    {/if}

    <!-- æœ€åå¥åº·æ£€æŸ¥ -->
    {#if dataSource.last_health_check}
      <div class="text-xs text-base-content/50 mb-4">
        æœ€åæ£€æŸ¥: {formatDateTime(dataSource.last_health_check)}
      </div>
    {/if}

    <!-- æ“ä½œæŒ‰é’® -->
    <div class="card-actions justify-end">
      <div class="join">
        <button 
          class="btn btn-sm btn-outline join-item"
          on:click={() => dispatch('test')}
        >
          æµ‹è¯•è¿æ¥
        </button>
        
        <button 
          class="btn btn-sm btn-outline join-item"
          on:click={() => dispatch('sync')}
        >
          åŒæ­¥å…ƒæ•°æ®
        </button>
        
        {#if $authStore.hasPermission('datasource', 'update')}
          <button 
            class="btn btn-sm btn-primary join-item"
            on:click={() => dispatch('edit')}
          >
            ç¼–è¾‘
          </button>
        {/if}
        
        {#if $authStore.hasPermission('datasource', 'delete')}
          <button 
            class="btn btn-sm btn-error join-item"
            on:click={() => dispatch('delete')}
          >
            åˆ é™¤
          </button>
        {/if}
      </div>
    </div>
  </div>
</div>

<style>
  .line-clamp-2 {
    display: -webkit-box;
    -webkit-line-clamp: 2;
    -webkit-box-orient: vertical;
    overflow: hidden;
  }
</style>
```

## ğŸ“‹ äº¤ä»˜æ¸…å•

### 1. åç«¯äº¤ä»˜ç‰©
- [ ] å®Œæ•´çš„æ•°æ®æºç®¡ç†æœåŠ¡
- [ ] å¤šæ•°æ®åº“é€‚é…å™¨å®ç°
- [ ] å…ƒæ•°æ®è‡ªåŠ¨åŒæ­¥ç³»ç»Ÿ
- [ ] æŸ¥è¯¢æ‰§è¡Œå¼•æ“
- [ ] æ•°æ®æƒé™æ§åˆ¶ç³»ç»Ÿ
- [ ] æŸ¥è¯¢ç›‘æ§å’Œå®¡è®¡åŠŸèƒ½
- [ ] å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

### 2. å‰ç«¯äº¤ä»˜ç‰©
- [ ] æ•°æ®æºç®¡ç†ç•Œé¢
- [ ] å…ƒæ•°æ®æµè§ˆå™¨
- [ ] æŸ¥è¯¢å†å²å’Œç›‘æ§ç•Œé¢
- [ ] æ•°æ®å­—å…¸ç®¡ç†ç•Œé¢
- [ ] æƒé™é…ç½®ç•Œé¢

### 3. æ•°æ®åº“äº¤ä»˜ç‰©
- [ ] å®Œæ•´çš„å…ƒæ•°æ®ç®¡ç†è¡¨ç»“æ„
- [ ] æ•°æ®æºè¿æ¥é…ç½®è¡¨
- [ ] æŸ¥è¯¢æ‰§è¡Œå†å²è¡¨
- [ ] æ•°æ®è¡€ç¼˜åˆ†æè¡¨
- [ ] æ•°æ®è´¨é‡ç®¡ç†è¡¨

### 4. æ–‡æ¡£äº¤ä»˜ç‰©
- [ ] æ•°æ®æ²»ç†æ¶æ„æ–‡æ¡£
- [ ] å¤šæ•°æ®æºæ¥å…¥æŒ‡å—
- [ ] å…ƒæ•°æ®ç®¡ç†æ‰‹å†Œ
- [ ] æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–æŒ‡å—
- [ ] æ•°æ®å®‰å…¨åˆè§„æ–‡æ¡£

### 5. éªŒæ”¶æ ‡å‡†
- [ ] æ”¯æŒ5+ä¸»æµæ•°æ®åº“ç±»å‹ï¼ˆ100%ï¼‰
- [ ] å…ƒæ•°æ®åŒæ­¥å‡†ç¡®ç‡ > 99%
- [ ] SQLç”Ÿæˆå‡†ç¡®ç‡æå‡åˆ°95%+
- [ ] æŸ¥è¯¢å“åº”æ—¶é—´é™ä½50%+
- [ ] æ•°æ®è®¿é—®100%å¯è¿½æº¯å®¡è®¡

---

**é¢„è®¡å®Œæˆæ—¶é—´**ï¼š3å‘¨ï¼ˆåœ¨ç”¨æˆ·ä½“ç³»åŸºç¡€ä¸Šå¢é‡å¼€å‘ï¼‰

**ä¾èµ–æ¡ä»¶**ï¼šç”¨æˆ·ä½“ç³»å·²å®Œæˆå¹¶ç¨³å®šè¿è¡Œ

*æœ¬æ–‡æ¡£ç”±æŠ€æœ¯æ¶æ„å›¢é˜Ÿç»´æŠ¤ï¼Œç‰ˆæœ¬ï¼šv1.0ï¼Œæœ€åæ›´æ–°ï¼š2025-01-08*