# ğŸ”— LangChainGoé›†æˆæŠ€æœ¯æŒ‡å—

<div align="center">

![LangChainGo](https://img.shields.io/badge/LangChainGo-v0.1.15-blue.svg)
![Go](https://img.shields.io/badge/Go-1.23+-00ADD8.svg)
![AI](https://img.shields.io/badge/AI-Production-green.svg)

**Chat2SQL P1é˜¶æ®µ - LangChainGoæ¡†æ¶é›†æˆå®æˆ˜æŒ‡å—**

</div>

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£ä¸“é—¨é’ˆå¯¹P1é˜¶æ®µLangChainGoæ¡†æ¶çš„é›†æˆå®ç°ï¼Œæä¾›è¯¦ç»†çš„æŠ€æœ¯æŒ‡å¯¼ã€æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ğŸ”§ æ ¸å¿ƒç»„ä»¶æ¶æ„

```go
// P1 LangChainGoé›†æˆæ¶æ„
type LangChainGoService struct {
    // æ ¸å¿ƒLLMå®¢æˆ·ç«¯
    primaryClient   llms.Model
    fallbackClient  llms.Model
    
    // é…ç½®ç®¡ç†
    config         *LLMConfig
    
    // è¿æ¥æ± ä¼˜åŒ–
    httpClient     *http.Client
    
    // ç›‘æ§å’ŒæŒ‡æ ‡
    metrics        *Metrics
    costTracker    *CostTracker
}
```

## ğŸ“¦ ä¾èµ–é…ç½®

### go.modé…ç½®

```go
module chat2sql-go

go 1.23

require (
    // LangChainGoæ ¸å¿ƒåº“
    github.com/tmc/langchaingo v0.1.15
    
    // LLMæä¾›å•†
    github.com/tmc/langchaingo/llms/openai v0.1.15
    github.com/tmc/langchaingo/llms/anthropic v0.1.15
    
    // ä¸“ç”¨ç»„ä»¶
    github.com/tmc/langchaingo/prompts v0.1.15
    github.com/tmc/langchaingo/chains v0.1.15
    github.com/tmc/langchaingo/memory v0.1.15
    github.com/tmc/langchaingo/embeddings v0.1.15
    
    // åŸºç¡€ä¾èµ–
    github.com/gin-gonic/gin v1.10.0
    github.com/jackc/pgx/v5 v5.6.0
)
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# LangChainGoç¯å¢ƒé…ç½®
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."

# æ¨¡å‹é…ç½®
export PRIMARY_MODEL_PROVIDER="openai"
export PRIMARY_MODEL_NAME="gpt-4o-mini"
export FALLBACK_MODEL_PROVIDER="anthropic"
export FALLBACK_MODEL_NAME="claude-3-haiku-20240307"

# æ€§èƒ½é…ç½®
export LLM_TIMEOUT="30s"
export LLM_MAX_RETRIES="3"
export LLM_MAX_TOKENS="2048"
```

## ğŸ”§ åˆå§‹åŒ–é…ç½®

### å®¢æˆ·ç«¯åˆå§‹åŒ–

```go
// internal/ai/langchain_service.go
package ai

import (
    "context"
    "fmt"
    "time"
    
    "github.com/tmc/langchaingo/llms"
    "github.com/tmc/langchaingo/llms/openai"
    "github.com/tmc/langchaingo/llms/anthropic"
)

type LangChainGoService struct {
    primaryClient  llms.Model
    fallbackClient llms.Model
    config         *Config
    metrics        *Metrics
}

func NewLangChainGoService(config *Config) (*LangChainGoService, error) {
    // åˆå§‹åŒ–ä¸»è¦æ¨¡å‹å®¢æˆ·ç«¯
    primaryClient, err := createClient(config.Primary)
    if err != nil {
        return nil, fmt.Errorf("åˆ›å»ºä¸»è¦æ¨¡å‹å®¢æˆ·ç«¯å¤±è´¥: %w", err)
    }
    
    // åˆå§‹åŒ–å¤‡ç”¨æ¨¡å‹å®¢æˆ·ç«¯
    fallbackClient, err := createClient(config.Fallback)
    if err != nil {
        return nil, fmt.Errorf("åˆ›å»ºå¤‡ç”¨æ¨¡å‹å®¢æˆ·ç«¯å¤±è´¥: %w", err)
    }
    
    return &LangChainGoService{
        primaryClient:  primaryClient,
        fallbackClient: fallbackClient,
        config:         config,
        metrics:        NewMetrics(),
    }, nil
}

func createClient(modelConfig ModelConfig) (llms.Model, error) {
    switch modelConfig.Provider {
    case "openai":
        return openai.New(
            openai.WithToken(modelConfig.APIKey),
            openai.WithModel(modelConfig.ModelName),
            openai.WithHTTPClient(&http.Client{
                Timeout: modelConfig.Timeout,
            }),
        )
    case "anthropic":
        return anthropic.New(
            anthropic.WithToken(modelConfig.APIKey),
            anthropic.WithModel(modelConfig.ModelName),
        )
    default:
        return nil, fmt.Errorf("ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: %s", modelConfig.Provider)
    }
}
```

### é…ç½®ç»“æ„ä½“

```go
// internal/ai/config.go
type Config struct {
    Primary  ModelConfig `yaml:"primary"`
    Fallback ModelConfig `yaml:"fallback"`
    
    // æ€§èƒ½é…ç½®
    MaxConcurrency int           `yaml:"max_concurrency"`
    Timeout        time.Duration `yaml:"timeout"`
    
    // æˆæœ¬æ§åˆ¶
    Budget BudgetConfig `yaml:"budget"`
}

type ModelConfig struct {
    Provider    string        `yaml:"provider"`
    ModelName   string        `yaml:"model_name"`
    APIKey      string        `yaml:"api_key"`
    Temperature float64       `yaml:"temperature"`
    MaxTokens   int           `yaml:"max_tokens"`
    TopP        float64       `yaml:"top_p"`
    Timeout     time.Duration `yaml:"timeout"`
}

type BudgetConfig struct {
    DailyLimit   float64 `yaml:"daily_limit"`   // æ¯æ—¥é¢„ç®—ä¸Šé™ï¼ˆç¾å…ƒï¼‰
    UserLimit    float64 `yaml:"user_limit"`    // æ¯ç”¨æˆ·é™åˆ¶
    AlertThreshold float64 `yaml:"alert_threshold"` // å‘Šè­¦é˜ˆå€¼
}
```

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½å®ç°

### SQLç”ŸæˆæœåŠ¡

```go
// internal/ai/sql_generator.go
func (lc *LangChainGoService) GenerateSQL(
    ctx context.Context, 
    req *SQLGenerationRequest) (*SQLGenerationResponse, error) {
    
    start := time.Now()
    
    // 1. æ„å»ºæç¤ºè¯
    prompt, err := lc.buildPrompt(req)
    if err != nil {
        return nil, err
    }
    
    // 2. è°ƒç”¨LangChainGoç”Ÿæˆå†…å®¹
    response, err := lc.callWithFallback(ctx, prompt)
    if err != nil {
        return nil, err
    }
    
    // 3. è§£æå“åº”
    sql, confidence := lc.parseResponse(response)
    
    // 4. è®°å½•æŒ‡æ ‡
    duration := time.Since(start)
    lc.metrics.RecordGeneration(duration, response.Usage.TotalTokens)
    
    return &SQLGenerationResponse{
        SQL:        sql,
        Confidence: confidence,
        TokensUsed: response.Usage.TotalTokens,
        Duration:   duration,
    }, nil
}

func (lc *LangChainGoService) callWithFallback(
    ctx context.Context, 
    prompt string) (*llms.ContentResponse, error) {
    
    // é¦–å…ˆå°è¯•ä¸»è¦æ¨¡å‹
    response, err := lc.primaryClient.GenerateContent(ctx, 
        []llms.MessageContent{
            llms.TextParts(llms.ChatMessageTypeHuman, prompt),
        },
        llms.WithTemperature(lc.config.Primary.Temperature),
        llms.WithMaxTokens(lc.config.Primary.MaxTokens),
    )
    
    if err == nil {
        lc.metrics.RecordSuccess("primary")
        return response, nil
    }
    
    // ä¸»è¦æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹
    lc.metrics.RecordFailure("primary", err)
    
    response, err = lc.fallbackClient.GenerateContent(ctx,
        []llms.MessageContent{
            llms.TextParts(llms.ChatMessageTypeHuman, prompt),
        },
        llms.WithTemperature(lc.config.Fallback.Temperature),
        llms.WithMaxTokens(lc.config.Fallback.MaxTokens),
    )
    
    if err != nil {
        lc.metrics.RecordFailure("fallback", err)
        return nil, fmt.Errorf("ä¸»è¦å’Œå¤‡ç”¨æ¨¡å‹éƒ½å¤±è´¥: %w", err)
    }
    
    lc.metrics.RecordSuccess("fallback")
    return response, nil
}
```

### å¹¶å‘å¤„ç†ä¼˜åŒ–

```go
// internal/ai/concurrent_processor.go
type ConcurrentProcessor struct {
    langChainService *LangChainGoService
    workerPool       *WorkerPool
    objectPool       sync.Pool
}

func (cp *ConcurrentProcessor) ProcessBatch(
    ctx context.Context, 
    requests []*SQLGenerationRequest) ([]*SQLGenerationResponse, error) {
    
    var wg sync.WaitGroup
    results := make([]*SQLGenerationResponse, len(requests))
    semaphore := make(chan struct{}, cp.workerPool.MaxWorkers)
    
    for i, req := range requests {
        wg.Add(1)
        go func(index int, request *SQLGenerationRequest) {
            defer wg.Done()
            
            // å¹¶å‘æ§åˆ¶
            semaphore <- struct{}{}
            defer func() { <-semaphore }()
            
            // å¯¹è±¡é‡ç”¨
            processor := cp.objectPool.Get().(*RequestProcessor)
            defer func() {
                processor.Reset()
                cp.objectPool.Put(processor)
            }()
            
            // å¤„ç†è¯·æ±‚
            response, err := cp.langChainService.GenerateSQL(ctx, request)
            if err != nil {
                results[index] = &SQLGenerationResponse{Error: err}
                return
            }
            
            results[index] = response
        }(i, req)
    }
    
    wg.Wait()
    return results, nil
}
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### HTTPå®¢æˆ·ç«¯ä¼˜åŒ–

```go
// internal/ai/http_optimization.go
func CreateOptimizedHTTPClient() *http.Client {
    return &http.Client{
        Transport: &http.Transport{
            MaxIdleConns:        100,              // æœ€å¤§ç©ºé—²è¿æ¥æ•°
            MaxIdleConnsPerHost: 10,               // æ¯ä¸ªhostæœ€å¤§ç©ºé—²è¿æ¥æ•°
            IdleConnTimeout:     90 * time.Second, // ç©ºé—²è¿æ¥è¶…æ—¶
            DisableCompression:  false,            // å¯ç”¨å‹ç¼©
            WriteBufferSize:     64 * 1024,        // å†™ç¼“å†²åŒº
            ReadBufferSize:      64 * 1024,        // è¯»ç¼“å†²åŒº
            ForceAttemptHTTP2:   true,             // å¼ºåˆ¶HTTP/2
        },
        Timeout: 30 * time.Second,
    }
}
```

### å¯¹è±¡æ± ä¼˜åŒ–

```go
// internal/ai/object_pool.go
var (
    requestPool = sync.Pool{
        New: func() interface{} {
            return &SQLGenerationRequest{}
        },
    }
    
    responsePool = sync.Pool{
        New: func() interface{} {
            return &SQLGenerationResponse{}
        },
    }
)

func GetRequest() *SQLGenerationRequest {
    return requestPool.Get().(*SQLGenerationRequest)
}

func PutRequest(req *SQLGenerationRequest) {
    req.Reset()
    requestPool.Put(req)
}
```

## ğŸ” ç›‘æ§å’ŒæŒ‡æ ‡

### PrometheusæŒ‡æ ‡

```go
// internal/ai/metrics.go
var (
    llmRequestsTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "langchaingo_requests_total",
            Help: "Total LangChainGo requests",
        },
        []string{"provider", "model", "status"},
    )
    
    llmRequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "langchaingo_request_duration_seconds",
            Help: "LangChainGo request duration",
            Buckets: []float64{0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0},
        },
        []string{"provider", "model"},
    )
    
    llmTokensUsed = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "langchaingo_tokens_used_total",
            Help: "Total tokens used",
        },
        []string{"provider", "model", "type"}, // type: input/output
    )
)
```

## ğŸ› é”™è¯¯å¤„ç†

### é‡è¯•æœºåˆ¶

```go
// internal/ai/retry.go
type RetryConfig struct {
    MaxRetries int
    BaseDelay  time.Duration
    MaxDelay   time.Duration
    Multiplier float64
}

func (lc *LangChainGoService) GenerateWithRetry(
    ctx context.Context, 
    req *SQLGenerationRequest) (*SQLGenerationResponse, error) {
    
    var lastErr error
    
    for attempt := 0; attempt <= lc.config.Retry.MaxRetries; attempt++ {
        response, err := lc.GenerateSQL(ctx, req)
        if err == nil {
            return response, nil
        }
        
        lastErr = err
        
        // æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
        if !shouldRetry(err) {
            break
        }
        
        // è®¡ç®—å»¶è¿Ÿæ—¶é—´
        delay := calculateDelay(attempt, lc.config.Retry)
        
        select {
        case <-time.After(delay):
            continue
        case <-ctx.Done():
            return nil, ctx.Err()
        }
    }
    
    return nil, fmt.Errorf("é‡è¯•%dæ¬¡åä»å¤±è´¥: %w", 
        lc.config.Retry.MaxRetries, lastErr)
}

func shouldRetry(err error) bool {
    // æ£€æŸ¥é”™è¯¯ç±»å‹ï¼Œå†³å®šæ˜¯å¦é‡è¯•
    if strings.Contains(err.Error(), "rate limit") {
        return true
    }
    if strings.Contains(err.Error(), "timeout") {
        return true
    }
    if strings.Contains(err.Error(), "connection refused") {
        return true
    }
    return false
}
```

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```go
// internal/ai/langchain_service_test.go
func TestLangChainGoService_GenerateSQL(t *testing.T) {
    mockClient := &MockLLMClient{}
    service := &LangChainGoService{
        primaryClient: mockClient,
        config:       defaultTestConfig(),
        metrics:      NewTestMetrics(),
    }
    
    testCases := []struct {
        name          string
        request       *SQLGenerationRequest
        mockResponse  *llms.ContentResponse
        mockError     error
        expectedSQL   string
        expectedError bool
    }{
        {
            name: "æˆåŠŸç”ŸæˆSQL",
            request: &SQLGenerationRequest{
                Query:  "æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·",
                Schema: testSchema,
            },
            mockResponse: &llms.ContentResponse{
                Choices: []llms.ContentChoice{
                    {Content: "SELECT * FROM users"},
                },
                Usage: llms.Usage{TotalTokens: 50},
            },
            expectedSQL: "SELECT * FROM users",
        },
        {
            name: "APIé”™è¯¯",
            request: &SQLGenerationRequest{
                Query: "æŸ¥è¯¢ç”¨æˆ·",
            },
            mockError:     errors.New("API rate limit"),
            expectedError: true,
        },
    }
    
    for _, tc := range testCases {
        t.Run(tc.name, func(t *testing.T) {
            mockClient.SetResponse(tc.mockResponse, tc.mockError)
            
            response, err := service.GenerateSQL(context.Background(), tc.request)
            
            if tc.expectedError {
                assert.Error(t, err)
            } else {
                assert.NoError(t, err)
                assert.Equal(t, tc.expectedSQL, response.SQL)
            }
        })
    }
}
```

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: LangChainGoå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥

**é—®é¢˜**: `failed to create OpenAI client: invalid API key`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $OPENAI_API_KEY

# ç¡®ä¿APIå¯†é’¥æ ¼å¼æ­£ç¡®
export OPENAI_API_KEY="sk-proj-..."  # æ–°æ ¼å¼
```

### Q2: è¯·æ±‚è¶…æ—¶

**é—®é¢˜**: `context deadline exceeded`

**è§£å†³æ–¹æ¡ˆ**:
```go
// å¢åŠ è¶…æ—¶æ—¶é—´
ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
defer cancel()

// æˆ–è°ƒæ•´å®¢æˆ·ç«¯é…ç½®
client := openai.New(
    openai.WithHTTPClient(&http.Client{
        Timeout: 60 * time.Second,
    }),
)
```

### Q3: Tokenä½¿ç”¨é‡è¿‡é«˜

**é—®é¢˜**: æˆæœ¬æ§åˆ¶å¤±æ•ˆ

**è§£å†³æ–¹æ¡ˆ**:
```go
// æ·»åŠ Tokené¢„æ£€æŸ¥
if estimatedTokens > maxTokensPerRequest {
    return nil, errors.New("è¯·æ±‚Tokenæ•°é‡è¶…é™")
}

// ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
config.Primary.ModelName = "gpt-4o-mini"  // è€Œä¸æ˜¯ "gpt-4"
```

### Q4: å¹¶å‘è¯·æ±‚é™åˆ¶

**é—®é¢˜**: `rate limit exceeded`

**è§£å†³æ–¹æ¡ˆ**:
```go
// æ·»åŠ é€Ÿç‡é™åˆ¶å™¨
rateLimiter := rate.NewLimiter(rate.Limit(10), 1) // æ¯ç§’10è¯·æ±‚

// åœ¨è¯·æ±‚å‰æ£€æŸ¥
if err := rateLimiter.Wait(ctx); err != nil {
    return nil, err
}
```

## ğŸ“š æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†

```go
// ä½¿ç”¨é…ç½®æ–‡ä»¶è€Œä¸æ˜¯ç¡¬ç¼–ç 
config, err := LoadConfig("configs/langchaingo.yaml")
if err != nil {
    log.Fatal("é…ç½®åŠ è½½å¤±è´¥", err)
}
```

### 2. é”™è¯¯å¤„ç†

```go
// æ€»æ˜¯å¤„ç†LangChainGoé”™è¯¯
response, err := client.GenerateContent(ctx, messages)
if err != nil {
    // è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
    log.Error("LangChainGoè°ƒç”¨å¤±è´¥", 
        zap.String("model", modelName),
        zap.Error(err))
    return nil, err
}
```

### 3. èµ„æºæ¸…ç†

```go
// ç¡®ä¿é€‚å½“æ¸…ç†èµ„æº
defer func() {
    if closer, ok := client.(io.Closer); ok {
        closer.Close()
    }
}()
```

### 4. ç›‘æ§é›†æˆ

```go
// æ¯ä¸ªè¯·æ±‚éƒ½è¦è®°å½•æŒ‡æ ‡
defer func(start time.Time) {
    duration := time.Since(start)
    metrics.RecordRequestDuration(duration)
    metrics.RecordTokenUsage(response.Usage.TotalTokens)
}(time.Now())
```

---

<div align="center">

**ğŸ”— LangChainGoé›†æˆæˆåŠŸçš„å…³é”®ï¼šåˆç†é…ç½® + é”™è¯¯å¤„ç† + æ€§èƒ½ç›‘æ§**

</div>