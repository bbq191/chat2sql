# ğŸ”„ è‡ªåŠ¨åŒ–è¿ç»´ä¸DevOpsæŒ‡å—

## ğŸ¯ æŠ€æœ¯æ¦‚è¿°

è‡ªåŠ¨åŒ–è¿ç»´ä½“ç³»ä¸ºChat2SQLæä¾›äº†å®Œæ•´çš„DevOpsèƒ½åŠ›ï¼Œé€šè¿‡CI/CDæµæ°´çº¿ã€è‡ªåŠ¨åŒ–éƒ¨ç½²ã€æ•…éšœè‡ªæ„ˆç­‰æŠ€æœ¯å®ç°é«˜æ•ˆè¿ç»´ã€‚æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»P5é˜¶æ®µç¬¬4å‘¨çš„è‡ªåŠ¨åŒ–è¿ç»´ä¸DevOpså®ç°ç­–ç•¥ã€‚

### âœ¨ æ ¸å¿ƒä»·å€¼

| åŠŸèƒ½ç‰¹æ€§ | æŠ€æœ¯å®ç° | ä¸šåŠ¡ä»·å€¼ | æ•ˆç‡æå‡ |
|---------|---------|---------|---------| 
| **CI/CDæµæ°´çº¿** | GitHub Actions + GitOps | å¿«é€Ÿè¿­ä»£éƒ¨ç½² | éƒ¨ç½²æ•ˆç‡æå‡80% |
| **è‡ªåŠ¨åŒ–å¤‡ä»½** | CronJob + S3 + Velero | æ•°æ®å®‰å…¨ä¿éšœ | æ¢å¤æ—¶é—´å‡å°‘90% |
| **æ•…éšœè‡ªæ„ˆ** | Kubernetes + è‡ªå®šä¹‰æ§åˆ¶å™¨ | ç³»ç»Ÿç¨³å®šæ€§ | æ•…éšœæ¢å¤æ—¶é—´å‡å°‘85% |
| **æ€§èƒ½ä¼˜åŒ–** | HPA/VPA + æ™ºèƒ½è°ƒåº¦ | èµ„æºåˆ©ç”¨ä¼˜åŒ– | æˆæœ¬èŠ‚çœ40% |

### ğŸ è¿ç»´åœºæ™¯

- **æŒç»­é›†æˆ**ï¼šä»£ç æäº¤è‡ªåŠ¨è§¦å‘æ„å»ºã€æµ‹è¯•ã€éƒ¨ç½²æµç¨‹
- **ç¯å¢ƒç®¡ç†**ï¼šå¤šç¯å¢ƒè‡ªåŠ¨åŒ–éƒ¨ç½²ã€é…ç½®ç®¡ç†ã€ç‰ˆæœ¬æ§åˆ¶
- **ç›‘æ§å‘Šè­¦**ï¼šå…¨æ–¹ä½ç›‘æ§ã€æ™ºèƒ½å‘Šè­¦ã€è‡ªåŠ¨åŒ–å“åº”
- **ç¾éš¾æ¢å¤**ï¼šæ•°æ®å¤‡ä»½ã€æ•…éšœåˆ‡æ¢ã€ä¸šåŠ¡è¿ç»­æ€§ä¿éšœ

---

## ğŸš€ CI/CDæµæ°´çº¿

### ğŸ“¦ GitHub Actionså·¥ä½œæµ

```yaml
# .github/workflows/ci-cd.yml
name: Chat2SQL CI/CD Pipeline

on:
  push:
    branches: [main, develop]
    tags: ['v*']
  pull_request:
    branches: [main, develop]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: chat2sql/backend
  HELM_CHART_PATH: ./charts/chat2sql

jobs:
  # ä»£ç è´¨é‡æ£€æŸ¥
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Go
      uses: actions/setup-go@v4
      with:
        go-version: '1.23'
        
    - name: Cache Go modules
      uses: actions/cache@v3
      with:
        path: ~/go/pkg/mod
        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
        
    - name: Run golangci-lint
      uses: golangci/golangci-lint-action@v3
      with:
        version: latest
        args: --timeout=5m
        
    - name: Run tests
      run: |
        go test -v -race -coverprofile=coverage.out ./...
        go tool cover -html=coverage.out -o coverage.html
        
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.out
        
    - name: Security scan
      uses: securecodewarrior/github-action-add-sarif@v1
      with:
        sarif-file: gosec-report.sarif

  # æ„å»ºå’Œæ¨é€é•œåƒ
  build-and-push:
    needs: code-quality
    runs-on: ubuntu-latest
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up QEMU
      uses: docker/setup-qemu-action@v3
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Login to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix=commit-
          
    - name: Build and push
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          VERSION=${{ github.ref_name }}
          COMMIT=${{ github.sha }}
          BUILD_TIME=${{ github.event.head_commit.timestamp }}

  # å®¹å™¨å®‰å…¨æ‰«æ
  security-scan:
    needs: build-and-push
    runs-on: ubuntu-latest
    steps:
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ needs.build-and-push.outputs.image-tag }}
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  # Helm ChartéªŒè¯
  helm-validation:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: 'v3.15.0'
        
    - name: Lint Helm Chart
      run: |
        helm dependency update ${{ env.HELM_CHART_PATH }}
        helm lint ${{ env.HELM_CHART_PATH }}
        
    - name: Template Helm Chart
      run: |
        helm template test ${{ env.HELM_CHART_PATH }} \
          --values ${{ env.HELM_CHART_PATH }}/values-test.yaml > rendered.yaml
        kubectl --dry-run=server --validate=true apply -f rendered.yaml

  # éƒ¨ç½²åˆ°å¼€å‘ç¯å¢ƒ
  deploy-dev:
    if: github.ref == 'refs/heads/develop'
    needs: [build-and-push, helm-validation, security-scan]
    runs-on: ubuntu-latest
    environment: development
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG_DEV }}
        
    - name: Deploy to development
      run: |
        helm upgrade --install chat2sql-dev ${{ env.HELM_CHART_PATH }} \
          --namespace chat2sql-dev \
          --create-namespace \
          --values ${{ env.HELM_CHART_PATH }}/values-dev.yaml \
          --set image.tag=${{ github.sha }} \
          --wait --timeout=10m
          
    - name: Run smoke tests
      run: |
        kubectl wait --for=condition=ready pod -l app=chat2sql-backend -n chat2sql-dev --timeout=300s
        kubectl exec -n chat2sql-dev deployment/chat2sql-backend -- /app/healthcheck
        
    - name: Deploy status notification
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  # éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
  deploy-prod:
    if: startsWith(github.ref, 'refs/tags/v')
    needs: [build-and-push, helm-validation, security-scan]
    runs-on: ubuntu-latest
    environment: production
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG_PROD }}
        
    - name: Create backup
      run: |
        kubectl create job --from=cronjob/postgresql-backup backup-pre-deploy-$(date +%Y%m%d-%H%M%S) -n chat2sql
        
    - name: Deploy to production
      run: |
        helm upgrade --install chat2sql-prod ${{ env.HELM_CHART_PATH }} \
          --namespace chat2sql \
          --values ${{ env.HELM_CHART_PATH }}/values-prod.yaml \
          --set image.tag=${{ github.ref_name }} \
          --wait --timeout=15m
          
    - name: Run production tests
      run: |
        kubectl wait --for=condition=ready pod -l app=chat2sql-backend -n chat2sql --timeout=600s
        ./scripts/production-health-check.sh
        
    - name: Rollback on failure
      if: failure()
      run: |
        helm rollback chat2sql-prod -n chat2sql
        
    - name: Production deployment notification
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#production-deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### ğŸ¯ GitOpså·¥ä½œæµ

```yaml
# .github/workflows/gitops.yml
name: GitOps Sync

on:
  push:
    branches: [main]
    paths: ['manifests/**', 'charts/**']

jobs:
  sync-gitops:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup ArgoCD CLI
      run: |
        curl -sSL https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 -o argocd
        chmod +x argocd
        sudo mv argocd /usr/local/bin/
        
    - name: Login to ArgoCD
      run: |
        argocd login ${{ secrets.ARGOCD_SERVER }} \
          --username ${{ secrets.ARGOCD_USERNAME }} \
          --password ${{ secrets.ARGOCD_PASSWORD }} \
          --insecure
          
    - name: Sync applications
      run: |
        argocd app sync chat2sql-prod --prune
        argocd app wait chat2sql-prod --timeout 600
        
    - name: Check application health
      run: |
        argocd app get chat2sql-prod -o json | jq '.status.health.status'
        
    - name: Notification
      if: always()
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        text: "GitOps sync completed for Chat2SQL"
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

---

## ğŸ’¾ è‡ªåŠ¨åŒ–å¤‡ä»½ä¸æ¢å¤

### ğŸ“¦ PostgreSQLè‡ªåŠ¨åŒ–å¤‡ä»½

```yaml
# postgresql-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgresql-backup
  namespace: chat2sql
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgresql-backup
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          containers:
          - name: backup
            image: postgres:17-alpine
            env:
            - name: PGHOST
              value: postgresql.chat2sql.svc.cluster.local
            - name: PGPORT
              value: "5432"
            - name: PGDATABASE
              value: chat2sql
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: postgresql-backup-secret
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgresql-backup-secret
                  key: password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-backup-secret
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-backup-secret
                  key: secret-access-key
            - name: S3_BUCKET
              value: chat2sql-backups
            - name: S3_REGION
              value: us-west-2
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
              BACKUP_FILE="chat2sql_backup_${BACKUP_DATE}.sql"
              
              echo "å¼€å§‹å¤‡ä»½æ•°æ®åº“..."
              
              # åˆ›å»ºæ•°æ®åº“å¤‡ä»½
              pg_dump --verbose --no-owner --no-privileges \
                --format=custom \
                --file=/tmp/${BACKUP_FILE} \
                ${PGDATABASE}
              
              # å‹ç¼©å¤‡ä»½æ–‡ä»¶
              gzip /tmp/${BACKUP_FILE}
              BACKUP_FILE="${BACKUP_FILE}.gz"
              
              # å®‰è£…AWS CLI
              apk add --no-cache aws-cli
              
              # ä¸Šä¼ åˆ°S3
              echo "ä¸Šä¼ å¤‡ä»½åˆ°S3..."
              aws s3 cp /tmp/${BACKUP_FILE} \
                s3://${S3_BUCKET}/postgresql/${BACKUP_FILE} \
                --region ${S3_REGION}
              
              # éªŒè¯ä¸Šä¼ 
              aws s3 ls s3://${S3_BUCKET}/postgresql/${BACKUP_FILE} --region ${S3_REGION}
              
              echo "å¤‡ä»½å®Œæˆ: ${BACKUP_FILE}"
              
              # æ¸…ç†æœ¬åœ°æ–‡ä»¶
              rm -f /tmp/${BACKUP_FILE}
              
              # åˆ é™¤30å¤©å‰çš„å¤‡ä»½
              aws s3 ls s3://${S3_BUCKET}/postgresql/ --region ${S3_REGION} | \
                while read -r line; do
                  backup_date=$(echo $line | awk '{print $1" "$2}')
                  backup_file=$(echo $line | awk '{print $4}')
                  if [[ $(date -d "$backup_date" +%s) -lt $(date -d "30 days ago" +%s) ]]; then
                    echo "åˆ é™¤è¿‡æœŸå¤‡ä»½: $backup_file"
                    aws s3 rm s3://${S3_BUCKET}/postgresql/$backup_file --region ${S3_REGION}
                  fi
                done
            resources:
              requests:
                memory: 512Mi
                cpu: 250m
              limits:
                memory: 1Gi
                cpu: 500m
          - name: metrics-exporter
            image: prom/node-exporter:latest
            ports:
            - containerPort: 9100
            command: ["/bin/node_exporter", "--web.listen-address=:9100"]
```

### ğŸ”„ Veleroé›†ç¾¤å¤‡ä»½

```yaml
# velero-backup-schedule.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: chat2sql-daily-backup
  namespace: velero
spec:
  schedule: "0 3 * * *"  # æ¯å¤©å‡Œæ™¨3ç‚¹
  template:
    metadata:
      labels:
        backup-type: daily
    includedNamespaces:
    - chat2sql
    - monitoring
    - vault-system
    excludedResources:
    - events
    - events.events.k8s.io
    - backups.velero.io
    - restores.velero.io
    storageLocation: default
    volumeSnapshotLocations:
    - default
    ttl: 720h  # 30å¤©
    hooks:
      resources:
      - name: postgresql-backup-hook
        includedNamespaces:
        - chat2sql
        includedResources:
        - pods
        labelSelector:
          matchLabels:
            app: postgresql
        pre:
        - exec:
            container: postgresql
            command:
            - /bin/bash
            - -c
            - |
              echo "å¼€å§‹æ•°æ®åº“ä¸€è‡´æ€§æ£€æŸ¥..."
              psql -c "CHECKPOINT;" -d chat2sql
              psql -c "SELECT pg_start_backup('velero-backup', true);" -d chat2sql
        post:
        - exec:
            container: postgresql
            command:
            - /bin/bash
            - -c
            - |
              psql -c "SELECT pg_stop_backup();" -d chat2sql
              echo "æ•°æ®åº“å¤‡ä»½é’©å­å®Œæˆ"
---
# å‘¨å¤‡ä»½
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: chat2sql-weekly-backup
  namespace: velero
spec:
  schedule: "0 4 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨4ç‚¹
  template:
    metadata:
      labels:
        backup-type: weekly
    includedNamespaces:
    - chat2sql
    - monitoring
    - vault-system
    - istio-system
    - kube-system
    storageLocation: default
    ttl: 2160h  # 90å¤©
```

### ğŸ“Š å¤‡ä»½æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# restore-database.sh

set -e

BACKUP_DATE=${1:-$(date +%Y%m%d)}
S3_BUCKET="chat2sql-backups"
S3_REGION="us-west-2"
NAMESPACE="chat2sql"

echo "ğŸ”„ å¼€å§‹æ¢å¤Chat2SQLæ•°æ®åº“å¤‡ä»½..."

# éªŒè¯å‚æ•°
if [[ -z "$BACKUP_DATE" ]]; then
    echo "âŒ é”™è¯¯: è¯·æä¾›å¤‡ä»½æ—¥æœŸ (æ ¼å¼: YYYYMMDD)"
    exit 1
fi

# æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶
echo "ğŸ“‹ æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶..."
BACKUP_FILE=$(aws s3 ls s3://${S3_BUCKET}/postgresql/ --region ${S3_REGION} | \
    grep "${BACKUP_DATE}" | tail -1 | awk '{print $4}')

if [[ -z "$BACKUP_FILE" ]]; then
    echo "âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ—¥æœŸ ${BACKUP_DATE} çš„å¤‡ä»½æ–‡ä»¶"
    echo "å¯ç”¨å¤‡ä»½:"
    aws s3 ls s3://${S3_BUCKET}/postgresql/ --region ${S3_REGION}
    exit 1
fi

echo "âœ… æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶: ${BACKUP_FILE}"

# ç¡®è®¤æ¢å¤æ“ä½œ
read -p "âš ï¸  ç¡®è®¤è¦æ¢å¤æ•°æ®åº“åˆ° ${BACKUP_DATE} çš„çŠ¶æ€å—? (y/N): " confirm
if [[ $confirm != [yY] ]]; then
    echo "âŒ æ¢å¤æ“ä½œå·²å–æ¶ˆ"
    exit 0
fi

# åˆ›å»ºæ¢å¤ä½œä¸š
echo "ğŸš€ åˆ›å»ºæ¢å¤ä½œä¸š..."
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: postgresql-restore-${BACKUP_DATE}
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: restore
        image: postgres:17-alpine
        env:
        - name: PGHOST
          value: postgresql.chat2sql.svc.cluster.local
        - name: PGPORT
          value: "5432"
        - name: PGDATABASE
          value: chat2sql
        - name: PGUSER
          valueFrom:
            secretKeyRef:
              name: postgresql-backup-secret
              key: username
        - name: PGPASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-backup-secret
              key: password
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: s3-backup-secret
              key: access-key-id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: s3-backup-secret
              key: secret-access-key
        command:
        - /bin/bash
        - -c
        - |
          set -e
          
          # å®‰è£…AWS CLI
          apk add --no-cache aws-cli
          
          # ä¸‹è½½å¤‡ä»½æ–‡ä»¶
          echo "ğŸ“¥ ä¸‹è½½å¤‡ä»½æ–‡ä»¶..."
          aws s3 cp s3://${S3_BUCKET}/postgresql/${BACKUP_FILE} \
            /tmp/${BACKUP_FILE} --region ${S3_REGION}
          
          # è§£å‹å¤‡ä»½æ–‡ä»¶
          gunzip /tmp/${BACKUP_FILE}
          BACKUP_FILE=\${BACKUP_FILE%.gz}
          
          # åœæ­¢åº”ç”¨è¿æ¥
          echo "â¸ï¸  åœæ­¢åº”ç”¨è¿æ¥..."
          kubectl scale deployment chat2sql-backend --replicas=0 -n ${NAMESPACE} || true
          
          # ç­‰å¾…è¿æ¥å…³é—­
          sleep 30
          
          # åˆ›å»ºæ¢å¤å‰å¤‡ä»½
          echo "ğŸ’¾ åˆ›å»ºæ¢å¤å‰å¤‡ä»½..."
          pg_dump --format=custom --file=/tmp/pre_restore_backup.sql \${PGDATABASE}
          
          # æ¢å¤æ•°æ®åº“
          echo "ğŸ”„ æ¢å¤æ•°æ®åº“..."
          pg_restore --verbose --clean --if-exists \
            --no-owner --no-privileges \
            --dbname=\${PGDATABASE} /tmp/\${BACKUP_FILE}
          
          # éªŒè¯æ¢å¤
          echo "âœ… éªŒè¯æ•°æ®åº“æ¢å¤..."
          psql -c "SELECT COUNT(*) FROM users;" -d \${PGDATABASE}
          psql -c "SELECT version();" -d \${PGDATABASE}
          
          # é‡æ–°å¯åŠ¨åº”ç”¨
          echo "ğŸš€ é‡æ–°å¯åŠ¨åº”ç”¨..."
          kubectl scale deployment chat2sql-backend --replicas=3 -n ${NAMESPACE}
          
          echo "âœ… æ•°æ®åº“æ¢å¤å®Œæˆ!"
        resources:
          requests:
            memory: 1Gi
            cpu: 500m
          limits:
            memory: 2Gi
            cpu: 1000m
EOF

# ç­‰å¾…ä½œä¸šå®Œæˆ
echo "â³ ç­‰å¾…æ¢å¤ä½œä¸šå®Œæˆ..."
kubectl wait --for=condition=complete job/postgresql-restore-${BACKUP_DATE} \
  -n ${NAMESPACE} --timeout=1800s

# æ£€æŸ¥ä½œä¸šçŠ¶æ€
JOB_STATUS=$(kubectl get job postgresql-restore-${BACKUP_DATE} \
  -n ${NAMESPACE} -o jsonpath='{.status.conditions[0].type}')

if [[ "$JOB_STATUS" == "Complete" ]]; then
    echo "âœ… æ•°æ®åº“æ¢å¤æˆåŠŸå®Œæˆ!"
    
    # è¿è¡Œå¥åº·æ£€æŸ¥
    echo "ğŸ” è¿è¡Œåº”ç”¨å¥åº·æ£€æŸ¥..."
    kubectl wait --for=condition=ready pod -l app=chat2sql-backend \
      -n ${NAMESPACE} --timeout=300s
    
    kubectl exec -n ${NAMESPACE} deployment/chat2sql-backend -- \
      /app/healthcheck
    
    echo "ğŸ‰ æ¢å¤æ“ä½œå®Œå…¨æˆåŠŸ!"
else
    echo "âŒ æ¢å¤ä½œä¸šå¤±è´¥"
    kubectl logs job/postgresql-restore-${BACKUP_DATE} -n ${NAMESPACE}
    exit 1
fi

# æ¸…ç†æ¢å¤ä½œä¸š
read -p "ğŸ§¹ åˆ é™¤æ¢å¤ä½œä¸š? (Y/n): " cleanup
if [[ $cleanup != [nN] ]]; then
    kubectl delete job postgresql-restore-${BACKUP_DATE} -n ${NAMESPACE}
    echo "âœ… æ¸…ç†å®Œæˆ"
fi
```

---

## ğŸ¯ æ•…éšœè‡ªæ„ˆä¸è‡ªåŠ¨åŒ–è¿ç»´

### ğŸ“¦ è‡ªå®šä¹‰æ§åˆ¶å™¨

```go
// æ•…éšœè‡ªæ„ˆæ§åˆ¶å™¨
package controller

import (
    "context"
    "fmt"
    "time"
    
    appsv1 "k8s.io/api/apps/v1"
    corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/apimachinery/pkg/runtime"
    "k8s.io/client-go/kubernetes"
    ctrl "sigs.k8s.io/controller-runtime"
    "sigs.k8s.io/controller-runtime/pkg/client"
)

// è‡ªæ„ˆæ§åˆ¶å™¨
type SelfHealingController struct {
    client.Client
    Scheme    *runtime.Scheme
    K8sClient kubernetes.Interface
    metrics   *PrometheusMetrics
}

// PodçŠ¶æ€æ£€æŸ¥å’Œè‡ªæ„ˆ
func (r *SelfHealingController) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    log := ctrl.Log.WithValues("pod", req.NamespacedName)
    
    // è·å–Pod
    var pod corev1.Pod
    if err := r.Get(ctx, req.NamespacedName, &pod); err != nil {
        return ctrl.Result{}, client.IgnoreNotFound(err)
    }
    
    // æ£€æŸ¥PodçŠ¶æ€
    if r.isPodFailing(&pod) {
        log.Info("æ£€æµ‹åˆ°Podæ•…éšœ", "pod", pod.Name)
        
        if err := r.healPod(ctx, &pod); err != nil {
            log.Error(err, "Podè‡ªæ„ˆå¤±è´¥")
            r.metrics.RecordHealingFailure(pod.Namespace, pod.Name)
            return ctrl.Result{RequeueAfter: time.Minute * 5}, err
        }
        
        r.metrics.RecordHealingSuccess(pod.Namespace, pod.Name)
        log.Info("Podè‡ªæ„ˆæˆåŠŸ", "pod", pod.Name)
    }
    
    return ctrl.Result{RequeueAfter: time.Minute * 2}, nil
}

// æ£€æŸ¥Podæ˜¯å¦æ•…éšœ
func (r *SelfHealingController) isPodFailing(pod *corev1.Pod) bool {
    // æ£€æŸ¥é‡å¯æ¬¡æ•°
    for _, containerStatus := range pod.Status.ContainerStatuses {
        if containerStatus.RestartCount > 5 {
            return true
        }
        
        // æ£€æŸ¥å®¹å™¨çŠ¶æ€
        if containerStatus.State.Waiting != nil {
            reason := containerStatus.State.Waiting.Reason
            if reason == "CrashLoopBackOff" || reason == "ImagePullBackOff" {
                return true
            }
        }
    }
    
    // æ£€æŸ¥Podé˜¶æ®µ
    if pod.Status.Phase == corev1.PodFailed {
        return true
    }
    
    // æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
    if r.isResourceExhausted(pod) {
        return true
    }
    
    return false
}

// æ‰§è¡ŒPodè‡ªæ„ˆ
func (r *SelfHealingController) healPod(ctx context.Context, pod *corev1.Pod) error {
    log := ctrl.Log.WithValues("pod", pod.Name)
    
    // è·å–Deployment
    deployment, err := r.getOwnerDeployment(ctx, pod)
    if err != nil {
        return fmt.Errorf("è·å–Deploymentå¤±è´¥: %w", err)
    }
    
    if deployment == nil {
        log.Info("Podä¸æ˜¯Deploymentç®¡ç†ï¼Œè·³è¿‡è‡ªæ„ˆ")
        return nil
    }
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å®¹
    if r.shouldScaleUp(deployment) {
        if err := r.scaleUpDeployment(ctx, deployment); err != nil {
            return fmt.Errorf("æ‰©å®¹Deploymentå¤±è´¥: %w", err)
        }
        log.Info("å·²æ‰©å®¹Deployment", "deployment", deployment.Name)
    }
    
    // é‡å¯å¤±è´¥çš„Pod
    if err := r.restartPod(ctx, pod); err != nil {
        return fmt.Errorf("é‡å¯Podå¤±è´¥: %w", err)
    }
    
    // å‘é€å‘Šè­¦é€šçŸ¥
    r.sendHealingNotification(pod, "Podè‡ªæ„ˆæ“ä½œå·²æ‰§è¡Œ")
    
    return nil
}

// é‡å¯Pod
func (r *SelfHealingController) restartPod(ctx context.Context, pod *corev1.Pod) error {
    // åˆ é™¤Podï¼Œè®©Deploymenté‡æ–°åˆ›å»º
    if err := r.Delete(ctx, pod); err != nil {
        return fmt.Errorf("åˆ é™¤Podå¤±è´¥: %w", err)
    }
    
    // ç­‰å¾…æ–°Podå¯åŠ¨
    time.Sleep(30 * time.Second)
    
    // éªŒè¯æ–°PodçŠ¶æ€
    return r.waitForPodReady(ctx, pod.Namespace, pod.Labels)
}

// æ‰©å®¹Deployment
func (r *SelfHealingController) scaleUpDeployment(ctx context.Context, deployment *appsv1.Deployment) error {
    if deployment.Spec.Replicas == nil {
        return fmt.Errorf("Deploymentå‰¯æœ¬æ•°ä¸ºç©º")
    }
    
    currentReplicas := *deployment.Spec.Replicas
    newReplicas := currentReplicas + 1
    
    // é™åˆ¶æœ€å¤§å‰¯æœ¬æ•°
    if newReplicas > 10 {
        return fmt.Errorf("å·²è¾¾åˆ°æœ€å¤§å‰¯æœ¬æ•°é™åˆ¶")
    }
    
    deployment.Spec.Replicas = &newReplicas
    
    if err := r.Update(ctx, deployment); err != nil {
        return fmt.Errorf("æ›´æ–°Deploymentå¤±è´¥: %w", err)
    }
    
    return nil
}

// æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å®¹
func (r *SelfHealingController) shouldScaleUp(deployment *appsv1.Deployment) bool {
    if deployment.Spec.Replicas == nil {
        return false
    }
    
    currentReplicas := *deployment.Spec.Replicas
    readyReplicas := deployment.Status.ReadyReplicas
    
    // å¦‚æœå°±ç»ªå‰¯æœ¬æ•°å°‘äºæœŸæœ›å‰¯æœ¬æ•°çš„50%ï¼Œåˆ™æ‰©å®¹
    return float64(readyReplicas) < float64(currentReplicas)*0.5
}
```

### ğŸ”§ è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

```bash
#!/bin/bash
# auto-ops.sh - è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

set -e

NAMESPACE="chat2sql"
PROMETHEUS_URL="http://prometheus.monitoring.svc.cluster.local:9090"
SLACK_WEBHOOK_URL="https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"

# æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
check_system_health() {
    echo "ğŸ” æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€..."
    
    # æ£€æŸ¥PodçŠ¶æ€
    failed_pods=$(kubectl get pods -n $NAMESPACE --field-selector=status.phase=Failed -o name)
    if [[ -n "$failed_pods" ]]; then
        echo "âš ï¸  å‘ç°å¤±è´¥çš„Pod:"
        echo "$failed_pods"
        
        # è‡ªåŠ¨æ¸…ç†å¤±è´¥çš„Pod
        echo "$failed_pods" | xargs kubectl delete -n $NAMESPACE
        send_notification "ğŸ§¹ å·²æ¸…ç†å¤±è´¥çš„Pod: $failed_pods"
    fi
    
    # æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
    check_resource_usage
    
    # æ£€æŸ¥å­˜å‚¨ç©ºé—´
    check_storage_usage
    
    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    check_network_connectivity
}

# æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
check_resource_usage() {
    echo "ğŸ“Š æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ..."
    
    # æŸ¥è¯¢PrometheusæŒ‡æ ‡
    cpu_usage=$(curl -s "${PROMETHEUS_URL}/api/v1/query?query=avg(rate(container_cpu_usage_seconds_total{namespace=\"$NAMESPACE\"}[5m]))" | \
        jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    
    memory_usage=$(curl -s "${PROMETHEUS_URL}/api/v1/query?query=avg(container_memory_working_set_bytes{namespace=\"$NAMESPACE\"})/1024/1024/1024" | \
        jq -r '.data.result[0].value[1]' 2>/dev/null || echo "0")
    
    # CPUä½¿ç”¨ç‡æ£€æŸ¥
    if (( $(echo "$cpu_usage > 0.8" | bc -l) )); then
        echo "âš ï¸  CPUä½¿ç”¨ç‡è¿‡é«˜: ${cpu_usage}"
        auto_scale_up "high-cpu"
    fi
    
    # å†…å­˜ä½¿ç”¨ç‡æ£€æŸ¥
    if (( $(echo "$memory_usage > 8" | bc -l) )); then
        echo "âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: ${memory_usage}GB"
        auto_scale_up "high-memory"
    fi
}

# è‡ªåŠ¨æ‰©å®¹
auto_scale_up() {
    local reason=$1
    echo "ğŸ“ˆ è§¦å‘è‡ªåŠ¨æ‰©å®¹: $reason"
    
    deployments=$(kubectl get deployments -n $NAMESPACE -o name)
    for deployment in $deployments; do
        current_replicas=$(kubectl get $deployment -n $NAMESPACE -o jsonpath='{.spec.replicas}')
        max_replicas=10
        
        if [[ $current_replicas -lt $max_replicas ]]; then
            new_replicas=$((current_replicas + 1))
            kubectl scale $deployment --replicas=$new_replicas -n $NAMESPACE
            echo "âœ… æ‰©å®¹ $deployment: $current_replicas -> $new_replicas"
            
            send_notification "ğŸ“ˆ è‡ªåŠ¨æ‰©å®¹: $deployment ($reason)"
        fi
    done
}

# æ£€æŸ¥å­˜å‚¨ä½¿ç”¨æƒ…å†µ
check_storage_usage() {
    echo "ğŸ’¾ æ£€æŸ¥å­˜å‚¨ä½¿ç”¨æƒ…å†µ..."
    
    # æ£€æŸ¥PVCä½¿ç”¨æƒ…å†µ
    pvcs=$(kubectl get pvc -n $NAMESPACE -o json)
    echo "$pvcs" | jq -r '.items[] | select(.status.capacity.storage) | "\(.metadata.name) \(.status.capacity.storage)"' | \
    while read pvc_name capacity; do
        # è¿™é‡Œå¯ä»¥æ·»åŠ å­˜å‚¨ä½¿ç”¨ç‡æ£€æŸ¥é€»è¾‘
        echo "PVC: $pvc_name, å®¹é‡: $capacity"
    done
    
    # æ£€æŸ¥èŠ‚ç‚¹å­˜å‚¨ç©ºé—´
    kubectl top nodes | awk 'NR>1 {if($5+0 > 80) print "âš ï¸  èŠ‚ç‚¹å­˜å‚¨ä½¿ç”¨ç‡è¿‡é«˜: " $1 " " $5}'
}

# æ£€æŸ¥ç½‘ç»œè¿æ¥
check_network_connectivity() {
    echo "ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥..."
    
    # æ£€æŸ¥æœåŠ¡é—´è¿æ¥
    kubectl exec -n $NAMESPACE deployment/chat2sql-backend -- \
        nc -zv postgresql.chat2sql.svc.cluster.local 5432 || \
        echo "âŒ æ•°æ®åº“è¿æ¥å¤±è´¥"
    
    kubectl exec -n $NAMESPACE deployment/chat2sql-backend -- \
        nc -zv redis.chat2sql.svc.cluster.local 6379 || \
        echo "âŒ Redisè¿æ¥å¤±è´¥"
}

# è‡ªåŠ¨æ¸…ç†èµ„æº
cleanup_resources() {
    echo "ğŸ§¹ æ¸…ç†è¿‡æœŸèµ„æº..."
    
    # æ¸…ç†å®Œæˆçš„Job
    kubectl delete jobs -n $NAMESPACE --field-selector=status.successful=1 \
        $(kubectl get jobs -n $NAMESPACE -o jsonpath='{.items[?(@.status.completionTime<"'$(date -d '7 days ago' -Iseconds)'")].metadata.name}') 2>/dev/null || true
    
    # æ¸…ç†è¿‡æœŸçš„Pod
    kubectl delete pods -n $NAMESPACE --field-selector=status.phase=Succeeded \
        $(kubectl get pods -n $NAMESPACE -o jsonpath='{.items[?(@.status.startTime<"'$(date -d '1 day ago' -Iseconds)'")].metadata.name}') 2>/dev/null || true
    
    # æ¸…ç†è¿‡æœŸçš„Secretï¼ˆä¸´æ—¶è¯ä¹¦ç­‰ï¼‰
    kubectl get secrets -n $NAMESPACE -o json | jq -r '.items[] | select(.metadata.annotations."cert-manager.io/certificate-name") | select(.metadata.creationTimestamp < "'$(date -d '30 days ago' -Iseconds)'") | .metadata.name' | \
    xargs -I {} kubectl delete secret {} -n $NAMESPACE 2>/dev/null || true
}

# æ•°æ®åº“ç»´æŠ¤
database_maintenance() {
    echo "ğŸ—„ï¸  æ‰§è¡Œæ•°æ®åº“ç»´æŠ¤..."
    
    # æ•°æ®åº“è¿æ¥æ•°æ£€æŸ¥
    active_connections=$(kubectl exec -n $NAMESPACE deployment/postgresql -- \
        psql -t -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';" | tr -d ' ')
    
    if [[ $active_connections -gt 80 ]]; then
        echo "âš ï¸  æ•°æ®åº“è¿æ¥æ•°è¿‡å¤š: $active_connections"
        # ç»ˆæ­¢é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢
        kubectl exec -n $NAMESPACE deployment/postgresql -- \
            psql -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'active' AND query_start < now() - interval '10 minutes';"
    fi
    
    # æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
    kubectl exec -n $NAMESPACE deployment/postgresql -- \
        psql -c "ANALYZE;" chat2sql
    
    # æ¸…ç†è¿‡æœŸçš„æŸ¥è¯¢å†å²
    kubectl exec -n $NAMESPACE deployment/postgresql -- \
        psql -c "DELETE FROM query_history WHERE created_at < now() - interval '90 days';" chat2sql
}

# å‘é€é€šçŸ¥
send_notification() {
    local message=$1
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    curl -X POST -H 'Content-type: application/json' \
        --data "{\"text\":\"[$timestamp] Chat2SQLè‡ªåŠ¨è¿ç»´: $message\"}" \
        $SLACK_WEBHOOK_URL || true
}

# ä¸»å‡½æ•°
main() {
    echo "ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–è¿ç»´æ£€æŸ¥ - $(date)"
    
    check_system_health
    cleanup_resources
    database_maintenance
    
    echo "âœ… è‡ªåŠ¨åŒ–è¿ç»´æ£€æŸ¥å®Œæˆ - $(date)"
    send_notification "âœ… è‡ªåŠ¨åŒ–è¿ç»´æ£€æŸ¥å®Œæˆ"
}

# å¦‚æœè„šæœ¬è¢«ç›´æ¥æ‰§è¡Œ
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸æ™ºèƒ½æ‰©ç¼©å®¹

### ğŸ“¦ KEDAè‡ªå®šä¹‰æ‰©ç¼©å®¹

```yaml
# keda-scaledobject.yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: chat2sql-backend-scaler
  namespace: chat2sql
spec:
  scaleTargetRef:
    name: chat2sql-backend
  minReplicaCount: 2
  maxReplicaCount: 20
  cooldownPeriod: 300
  pollingInterval: 30
  triggers:
  # åŸºäºCPUä½¿ç”¨ç‡
  - type: cpu
    metadata:
      type: Utilization
      value: "70"
  # åŸºäºå†…å­˜ä½¿ç”¨ç‡
  - type: memory
    metadata:
      type: Utilization
      value: "80"
  # åŸºäºHTTPè¯·æ±‚é˜Ÿåˆ—é•¿åº¦
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: http_requests_per_second
      threshold: "50"
      query: sum(rate(http_requests_total{job="chat2sql-backend"}[2m]))
  # åŸºäºæ•°æ®åº“è¿æ¥æ•°
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: database_connections_active
      threshold: "80"
      query: sum(chat2sql_db_connections_active{pool="main"})
  # åŸºäºAIæ¨ç†é˜Ÿåˆ—é•¿åº¦
  - type: redis
    metadata:
      address: redis.chat2sql.svc.cluster.local:6379
      listName: ai_inference_queue
      listLength: "10"
      enableTLS: "false"
---
# æ•°æ®åº“æ‰©ç¼©å®¹
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: postgresql-readonly-scaler
  namespace: chat2sql
spec:
  scaleTargetRef:
    name: postgresql-readonly
  minReplicaCount: 1
  maxReplicaCount: 5
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: database_cpu_usage
      threshold: "60"
      query: avg(rate(container_cpu_usage_seconds_total{pod=~"postgresql-readonly-.*"}[5m])) * 100
```

### ğŸ¯ VPAé…ç½®

```yaml
# vpa-recommender.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: chat2sql-backend-vpa
  namespace: chat2sql
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: chat2sql-backend
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: chat2sql
      minAllowed:
        cpu: 100m
        memory: 256Mi
      maxAllowed:
        cpu: 2000m
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
---
# PostgreSQL VPA
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: postgresql-vpa
  namespace: chat2sql
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: postgresql
  updatePolicy:
    updateMode: "Off"  # ä»…æ¨èï¼Œä¸è‡ªåŠ¨æ›´æ–°
  resourcePolicy:
    containerPolicies:
    - containerName: postgresql
      minAllowed:
        cpu: 500m
        memory: 1Gi
      maxAllowed:
        cpu: 4000m
        memory: 16Gi
```

---

## ğŸ§ª æ··æ²Œå·¥ç¨‹ä¸éŸ§æ€§æµ‹è¯•

### ğŸ“¦ Chaos Meshå®éªŒ

```yaml
# chaos-experiments.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: chat2sql-pod-failure
  namespace: chat2sql
spec:
  action: pod-kill
  mode: one
  duration: "30s"
  scheduler:
    cron: "0 */6 * * *"  # æ¯6å°æ—¶æ‰§è¡Œä¸€æ¬¡
  selector:
    namespaces:
    - chat2sql
    labelSelectors:
      "app": "chat2sql-backend"
---
# ç½‘ç»œå»¶è¿Ÿå®éªŒ
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: chat2sql-network-delay
  namespace: chat2sql
spec:
  action: delay
  mode: one
  duration: "5m"
  scheduler:
    cron: "0 2 * * 1"  # æ¯å‘¨ä¸€å‡Œæ™¨2ç‚¹
  selector:
    namespaces:
    - chat2sql
    labelSelectors:
      "app": "chat2sql-backend"
  delay:
    latency: "100ms"
    correlation: "100"
    jitter: "10ms"
  direction: to
  target:
    mode: one
    selector:
      namespaces:
      - chat2sql
      labelSelectors:
        "app": "postgresql"
---
# ç£ç›˜IOå‹åŠ›æµ‹è¯•
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
metadata:
  name: chat2sql-disk-stress
  namespace: chat2sql
spec:
  mode: one
  duration: "10m"
  scheduler:
    cron: "0 3 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹
  selector:
    namespaces:
    - chat2sql
    labelSelectors:
      "app": "postgresql"
  stressors:
    iomix:
      workers: 2
      size: "1GB"
```

### ğŸ” éŸ§æ€§æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# resilience-test.sh

set -e

NAMESPACE="chat2sql"
TEST_DURATION="300"  # 5åˆ†é’Ÿ
CONCURRENT_USERS="50"

echo "ğŸ§ª å¼€å§‹éŸ§æ€§æµ‹è¯•..."

# è¿è¡Œè´Ÿè½½æµ‹è¯•
run_load_test() {
    echo "ğŸ“ˆ å¯åŠ¨è´Ÿè½½æµ‹è¯•..."
    
    kubectl run load-test --image=grafana/k6:latest --rm -i --restart=Never -- \
        run - <<EOF
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
    vus: $CONCURRENT_USERS,
    duration: '${TEST_DURATION}s',
    thresholds: {
        http_req_duration: ['p(95)<2000'],
        http_req_failed: ['rate<0.1'],
    },
};

export default function() {
    let response = http.post('http://chat2sql-backend.chat2sql.svc.cluster.local:8080/api/query', 
        JSON.stringify({
            query: "SELECT COUNT(*) FROM users WHERE active = true"
        }), 
        {
            headers: {
                'Content-Type': 'application/json',
                'Authorization': 'Bearer test-token'
            }
        }
    );
    
    check(response, {
        'status is 200': (r) => r.status === 200,
        'response time < 2s': (r) => r.timings.duration < 2000,
    });
    
    sleep(1);
}
EOF
}

# æ•…éšœæ³¨å…¥æµ‹è¯•
inject_failures() {
    echo "ğŸ’¥ æ³¨å…¥æ•…éšœè¿›è¡Œæµ‹è¯•..."
    
    # Podæ•…éšœ
    kubectl apply -f - <<EOF
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: resilience-test-pod-kill
  namespace: $NAMESPACE
spec:
  action: pod-kill
  mode: fixed-percent
  value: "33"
  duration: "60s"
  selector:
    namespaces:
    - $NAMESPACE
    labelSelectors:
      "app": "chat2sql-backend"
EOF
    
    sleep 60
    
    # ç½‘ç»œæ•…éšœ
    kubectl apply -f - <<EOF
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: resilience-test-network-partition
  namespace: $NAMESPACE
spec:
  action: partition
  mode: one
  duration: "60s"
  selector:
    namespaces:
    - $NAMESPACE
    labelSelectors:
      "app": "chat2sql-backend"
  direction: both
  target:
    mode: one
    selector:
      namespaces:
      - $NAMESPACE
      labelSelectors:
        "app": "postgresql"
EOF
    
    sleep 60
}

# ç›‘æ§å’Œæ”¶é›†æŒ‡æ ‡
monitor_metrics() {
    echo "ğŸ“Š æ”¶é›†éŸ§æ€§æµ‹è¯•æŒ‡æ ‡..."
    
    # é”™è¯¯ç‡
    error_rate=$(kubectl exec -n monitoring deployment/prometheus -- \
        promtool query instant 'rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])' | \
        grep -o '[0-9.]*' | tail -1)
    
    # å¹³å‡å“åº”æ—¶é—´
    avg_response_time=$(kubectl exec -n monitoring deployment/prometheus -- \
        promtool query instant 'histogram_quantile(0.5, rate(http_request_duration_seconds_bucket[5m]))' | \
        grep -o '[0-9.]*' | tail -1)
    
    # æ¢å¤æ—¶é—´
    recovery_time=$(kubectl exec -n monitoring deployment/prometheus -- \
        promtool query instant 'time() - on() chat2sql_last_failure_timestamp' | \
        grep -o '[0-9.]*' | tail -1)
    
    echo "ğŸ“ˆ éŸ§æ€§æµ‹è¯•ç»“æœ:"
    echo "   é”™è¯¯ç‡: ${error_rate}%"
    echo "   å¹³å‡å“åº”æ—¶é—´: ${avg_response_time}s"
    echo "   æ¢å¤æ—¶é—´: ${recovery_time}s"
}

# æ¸…ç†æµ‹è¯•èµ„æº
cleanup() {
    echo "ğŸ§¹ æ¸…ç†æµ‹è¯•èµ„æº..."
    
    kubectl delete podchaos resilience-test-pod-kill -n $NAMESPACE --ignore-not-found
    kubectl delete networkchaos resilience-test-network-partition -n $NAMESPACE --ignore-not-found
    
    echo "âœ… æ¸…ç†å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    trap cleanup EXIT
    
    echo "ğŸš€ å¼€å§‹éŸ§æ€§æµ‹è¯• - $(date)"
    
    # å¹¶è¡Œè¿è¡Œè´Ÿè½½æµ‹è¯•å’Œæ•…éšœæ³¨å…¥
    run_load_test &
    LOAD_TEST_PID=$!
    
    sleep 60  # è®©è´Ÿè½½æµ‹è¯•å…ˆè¿è¡Œ1åˆ†é’Ÿ
    inject_failures
    
    wait $LOAD_TEST_PID
    
    monitor_metrics
    
    echo "âœ… éŸ§æ€§æµ‹è¯•å®Œæˆ - $(date)"
}

main "$@"
```

---

## ğŸ“š æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•

1. **CI/CDæµæ°´çº¿**
   - è‡ªåŠ¨åŒ–æµ‹è¯•è¦†ç›–ç‡>80%
   - å¤šç¯å¢ƒéƒ¨ç½²ç­–ç•¥
   - å›æ»šæœºåˆ¶å®Œå–„

2. **å¤‡ä»½ç­–ç•¥**
   - è‡ªåŠ¨åŒ–å®šæœŸå¤‡ä»½
   - è·¨åœ°åŸŸå¤‡ä»½å­˜å‚¨
   - å®šæœŸæ¢å¤æ¼”ç»ƒ

3. **ç›‘æ§å‘Šè­¦**
   - ä¸šåŠ¡æŒ‡æ ‡ä¼˜å…ˆ
   - åˆ†çº§å‘Šè­¦æœºåˆ¶
   - è‡ªåŠ¨åŒ–å“åº”

4. **æ•…éšœå¤„ç†**
   - é¢„å®šä¹‰å¤„ç†æµç¨‹
   - è‡ªåŠ¨åŒ–æ•…éšœæ¢å¤
   - æ•…éšœå¤ç›˜æœºåˆ¶

### âŒ é¿å…çš„é™·é˜±

1. **è¿‡åº¦è‡ªåŠ¨åŒ–**
   - é¿å…è‡ªåŠ¨åŒ–å…³é”®æ“ä½œæ— äººå®¡æ ¸
   - ä¿ç•™æ‰‹åŠ¨å¹²é¢„èƒ½åŠ›

2. **å¤‡ä»½ç–æ¼**
   - ä¸è¦å¿½è§†å¤‡ä»½éªŒè¯
   - é¿å…å•ç‚¹å¤‡ä»½å¤±è´¥

3. **å‘Šè­¦è¿‡è½½**
   - é¿å…å‘Šè­¦é£æš´
   - åˆç†è®¾ç½®å‘Šè­¦é˜ˆå€¼

---

## ğŸ”— ç›¸å…³èµ„æº

- **Kuberneteså®˜æ–¹è¿ç»´æŒ‡å—**ï¼šhttps://kubernetes.io/docs/tasks/
- **Prometheusç›‘æ§æœ€ä½³å®è·µ**ï¼šhttps://prometheus.io/docs/practices/
- **Chaos EngineeringåŸç†**ï¼šhttps://principlesofchaos.org/
- **GitOpså·¥ä½œæµæŒ‡å—**ï¼šhttps://www.gitops.tech/

---

ğŸ’¡ **å®æ–½å»ºè®®**ï¼šæŒ‰ç…§ç¬¬4å‘¨çš„å¼€å‘è®¡åˆ’ï¼Œå…ˆå»ºç«‹CI/CDæµæ°´çº¿ï¼Œç„¶åå®ç°è‡ªåŠ¨åŒ–å¤‡ä»½ã€æ•…éšœè‡ªæ„ˆæœºåˆ¶ï¼Œæœ€åè¿›è¡ŒéŸ§æ€§æµ‹è¯•éªŒè¯ï¼Œç¡®ä¿è¿ç»´è‡ªåŠ¨åŒ–ç¨‹åº¦è¾¾åˆ°90%ä»¥ä¸Šã€‚